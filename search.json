[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The R Companion to STATS2",
    "section": "",
    "text": "This is a guide to conducting the analysis demonstrated in Stat2: Modeling with Regression and ANOVA (Cannon et al. 2018) using R, with a focus on using tools and techniques from the Tidyverse.\nThis guide assumes some prior familiarity with R programming, so that it may focus on demonstrating the content from STAT2, rather than teaching R wholesale. Thankfully, there are many free resources introducing R programming and tidyverse packages the reader may take advantage of to gain this familiarity. Below are a few books which provide a mostly comprehensive introduction:\n\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse\nData Science: A First Introduction\nYaRrr! The Pirate’s Guide to R (focused more on ‘base’ R than the tidyverse, but still good.)\nHands-On Programming with R\n\n\n\n\n\nCannon, A. R., G. W. Cobb, B. A. Hartlaub, J. M. Legler, R. H. Lock, T. L. Moore, A. J. Rossman, and J. A. Witmer. 2018. Stat2: Modeling with Regression and ANOVA. Macmillan Learning. https://www.macmillanlearning.com/college/us/product/STAT2/p/1319054072."
  },
  {
    "objectID": "01_simple_linear_regression.html",
    "href": "01_simple_linear_regression.html",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "Linear regression is introduced in Chapter 1 with the motivating question:\n\nHow much should you expect to pay for a used Honda Accord, if you know how many miles the car has already been driven?\n\nand introduces the AccordPrice data set, which contains information about the list price and mileage for a sample of 30 Honda Accords. The AccordPrice data set is included with the Stat2Data R package, so to access the data for yourself, you’ll need to install the package. If you don’t already know how to install R packages, here are two good resources to walk you through the process:\n\nReading: ModernDive Chapter 1.3.1: Installing Packages\nWatching: How to Install Packages in R Studio on YouTube\n\nOnce you have the package installed, load the package into your R session using:\n\nlibrary(Stat2Data)\n\nTo load the AccordPrice data set into your R environment, use the command:\n\ndata(\"AccordPrice\")\n\n\n\n\n\n  \n\n\n\nAs a side note: not much information is given in the text about how this sample of 30 Accords was collected, but we can gather a bit more information by looking at the help page for the AccordPrice data set. To open the help page for the AccordPrice data set, you can run the command\n\n?AccordPrice\n\nin the R console. By reading the “Details” and “Source” sections, we can learn that these 30 Accords were listed for sale on Cars.com in Lakewood Ohio during February 2017. Whenever you want to to know more about one of the textbook’s data sets, the help page for that data set is a good place to look first. Sometimes there’s not much more information than given in the textbook, but every little bit helps!\n\n\nFigure 1.2 displays a scatter plot of the Mileage and Price variable, showing how those variables relate to one another. To re-produce this scatter plot, we’ll use the ggplot2 R package (Wickham 2016). If you’re not already familiar with the ggplot2 package, here are a few good resources to help you get started:\n\nReading: ModernDive Chapter 2: Data Visualization\nReading: Effective data visualization\nWatching: ggplot for plots and graphs on YouTube\n\nTo re-create this scatter plot, we’ll map the Mileage variable to x-axis aesthetic, and the Price variable to the y-axis aesthetic, and draw a layer of points to represent each of the 30 cars using geom_point()\n\nlibrary(ggplot2)\n\nggplot(data = AccordPrice,\n       mapping = aes(x=Mileage, y=Price)\n       ) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nAside\n\n\n\nIf you want to exactly reproduce the scatter plots in STAT2, right down to the colors, backgrounds, and fonts, you can use the following ggplot2 theme:\n\n\nCode\ntheme_stat2 <- function(base_size = 11,\n                        base_family = \"\",\n                        base_line_size = base_size/22,\n                        base_rect_size = base_size/22) {\n  \n  theme_bw() %+replace% \n  theme(axis.text.x = element_text(color=\"black\"),\n        axis.text.y = element_text(color=\"black\"),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background=element_rect(colour=\"black\"),\n        complete = TRUE\n        )\n}\n\n\nAnd use the hex color code #92278f for your geometric objects. For example, this could exactly reproduce Figure 1.2 by adapting the code above to use this new theme:\n\n\nCode\nggplot(data = AccordPrice,\n       mapping = aes(x=Mileage, y=Price)\n       ) +\n  geom_point(color=\"#92278f\") +\n  theme_stat2()\n\n\nIn the rest of this book, we won’t use the STAT2 theme for our visualizations, but provide it here for completeness.\n\n\n\n\n\nExample 1.3 shows a summary of a simple linear regression model fit to the Mileage and Price variable in the AccordPrices data set. This summary is actually a mix of two different summaries, a regression table and an Analysis of Variance (ANOVA) table. Reproducing this summary will be a 3 step process in R:\n\nFitting the model using the lm() function\nPrinting the regression table with the summary() function\nPrinting the ANOVA table with the anova() function\n\n\n\n\nThe lm() function (short for linear model) does the “heavy lifting’ of estimating the coefficients of the simple linear model. In other words, the lm() function find the optimal values for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) in the model \\(Price = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot Mileage + \\epsilon\\).\nTo fit a linear regression model using lm, you need to supply:\n\nA formula describing relationship between the outcome and explanatory variable(s)\nThe name of a data set where the outcome and explanatory variables can be found.\n\nIn this case, our call to the lm function would be:\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\n\nThe first argument inside the lm() function is the formula describing the structure of the model. In R, model formulas are always created using the ~ symbol, with the outcome variable named on the left, and the explanatory variables(s) named on the right. As you might notice, R’s model formula code is an adaptation of how the model is described in mathematical notation.\nAlso, take note that we’ve saved the results from fitting this linear model in a new R object named price_mileage_model. We’ll need to use this new object to produce the regression table and the ANOVA table in steps 2 and 3 below.\n\n\n\nIn order to report the regression table, we need to call the summary() function on the linear model object we just created:\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.8096     0.9529   21.84  < 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,    Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\nAs we can see, the summary() function first prints out a few things not shown as part of the summary in the textbook: a copy of the code used to fit the model, and a the Five-number summary of the model’s residual errors. These are followed by the regression table summarizing the intercept and slope, and a “goodness of fit” summary of the model as whole.\n\n\n\nThe ANOVA table is found by calling the aptly named anova() function on the linear model, the same way we just did with the summary() function a moment ago:\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nanova(price_mileage_model)\n\n\n\nAnalysis of Variance Table\n\nResponse: Price\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nMileage    1 687.66  687.66  72.253 3.055e-09 ***\nResiduals 28 266.49    9.52                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nFigure 1.3 shows the Price vs. Mileage scatter plot again, but this time with a line representing the regression model’s predictions drawn on top of the raw data. Surprisingly, the easiest method for visualizing the predictions of a regression model doesn’t involve the fitted model object. Instead, we will begin with the same ggplot code we used to draw the Mileage vs. Price scatter plot earlier, and add to it. The geom_smooth() function is used to draw the regression line on top of the raw data:\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE, formula = y~x)\n\n\n\n\ngeom_smooth() is a generic smoothing function: the key argument that tells it to fit and display a linear regression model is the method = lm argument. Without the method=lm argument, geom_smooth() will not display a linear model.\nThe se = FALSE argument is included to stop ggplot from drawing confidence interval bands around the regression line. And, the formula = y~x argument is included simply to prevent ggplot from printing an annoying message that says geom_smooth() using formula ‘y ~ x’ when creating the plot.\n\n\n\nExample 1.4 demonstrates how centering a variable (i.e., shifting all the values left or right by a single chosen number) changes the interpretation of the intercept coefficient, but not the slope coefficient. In this example, the Mileage variable is shifted to the left by 50; in other words, 50 is subtracted from all the Mileage values before fitting the model.\nThe easiest way to replicate this model is create a new variable in the AccordPrices data set which holds the centered Mileage values. To make this new column, we’ll use the mutate function from the dplyr package (Wickham et al. 2022). If you aren’t familiar with the mutate() function or the dplyr package, here are a few good resources to investigate:\n\nReading: ModernDive Chapter 3: Data Wrangling\nReading: Cleaning and Wrangling Data\nWatching: Dplyr Essentials on YouTube\n\nIn this case, the ‘mutation’ we apply is quite simple: we just use the subtraction operator to subtract 50, and R automatically applies this subtraction to all 30 values in the Mileage column.\n\nlibrary(dplyr)\n\nAccordPrice <- AccordPrice |>\n  mutate(Mileage_c50 = Mileage - 50)\nAccordPrice\n\n\n\n  \n\n\n\nNote that we saved our centered mileage scores in a variable named Mileage_c50, to help us keep track of what these values mean: they are mileage values that have been centered by 50.\nFrom here, we just need to fit another linear model with lm(), using our new Mileage_c50 variable as the explanatory variable in our model formula:\n\ncentered_mileage_model <- lm(Price ~ Mileage_c50, data = AccordPrice)\n\nThe textbook only presents the fitted model equation (not the full regression table) in order to show the intercept and slope coefficients. If you ever need just the coefficient values, without the rest of the summaries in the regression table, you can use the coef() function on your model object to print them out:\n\ncentered_mileage_model <- lm(Price ~ Mileage_c50, data = AccordPrice)\ncoef(centered_mileage_model)\n\n(Intercept) Mileage_c50 \n 14.8190154  -0.1198119 \n\n\n\n\n\nIf you are using a literate programming environment (like an RMarkdown or Quarto document), you might find yourself wanting to display the fitted model equation in your document, formatted like a “fancy” mathematical equation. You could always write the LaTeX markup you need yourself, but the equatiomatic package (Anderson, Heiss, and Sumners 2022) can automatically generate what you need, straight from the model object itself!\nTo demonstrate, let’s display a formatted equation representing the fitted regression model based on the centered mileage scores by using the extract_eq() function on the model object.\n\nlibrary(equatiomatic)\ncentered_mileage_model <- lm(Price ~ Mileage_c50, data = AccordPrice)\nextract_eq(centered_mileage_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{Price}} = 14.82 - 0.12(\\operatorname{Mileage\\_c50})\n\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs the time of writing, there are problems with using the equatiomatic package to display equations when rendering Quarto documents to PDF. Thankfully, there is a workaround that is not too difficult, which involves saving the equation as a variable, and cat()-ing the equation yourself:\n```{r}\n#| results: asis\neq <- extract_eq(centered_mileage_model, use_coefs = TRUE)\ncat(\"$$\", eq, \"$$\", sep = \"\\n\")\n```\nJust be sure to set the results: asis chunk option!"
  },
  {
    "objectID": "01_simple_linear_regression.html#conditions-for-a-simple-linear-model",
    "href": "01_simple_linear_regression.html#conditions-for-a-simple-linear-model",
    "title": "1  Simple Linear Regression",
    "section": "1.2 Conditions for a Simple Linear Model",
    "text": "1.2 Conditions for a Simple Linear Model\nSection 1.2 introduces mostly conceptual information about neccesary and sufficient conditions for interference on a linear model, but does introduce the formula for estimating the standard error of the regression (also called the “Residual Standard Error”). You’ll rarely need to use this formula “manually”, since the value of this statistic is included in the output from the summary() function:\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.8096     0.9529   21.84  < 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,    Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\n\nIf you ever need to obtain this value alone (without the rest of the summary table), you can use the sigma() function on the fitted model object to extract it;\n\nsigma(price_mileage_model)\n\n[1] 3.08504"
  },
  {
    "objectID": "01_simple_linear_regression.html#assessing-conditions",
    "href": "01_simple_linear_regression.html#assessing-conditions",
    "title": "1  Simple Linear Regression",
    "section": "1.3 Assessing Conditions",
    "text": "1.3 Assessing Conditions\nSection 1.4 introduces two type of plots that crucial for assessing the validity of the assumptions underpinning theory-based inference on a regression model:\n\nThe Fitted vs. Residuals Plot\nThe Normal Quantile plot (which is a specific type of Quantile-Quantile plot)\n\nThere are many ways of generating thse plots from a linear model in R, but perhaps the easiest, most full-featured (and prettiest!) method is to use the tools from the performance package (Lüdecke et al. 2021).\nThe check_model() function from the performance package can be used to create both fitted vs. residuals plots, and Normal Quantile plots. What’s more, it can create different variations of fitted vs. residuals plots that are customized to help you check either the Linearity or the Homogeneity of Variance assumptions.\n\n\n\n\n\n\nTip\n\n\n\nYou’ll also want to install the patchwork and see packages at the same time you isntall the performance package, as these supplemental packages are very useful for constructing visualiations with the performance package.\n\n\n\n1.3.1 Checking Linearity with a Fitted vs. Residuals plot\nTo generate a variant of the Fitted vs. Residuals plot designed to help you assess the Linearity assumption, you’ll include the check=\"linearity\" argument to the check_model() function. The panel=FALSE argument instructs the check_model() function to devote the entire plot window to this plot (instead of leaving room for other plots checking other model assumptions).\n\nlibrary(performance)\nlibrary(see)\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nlinearity_check <- check_model(price_mileage_model, check=\"linearity\",\n                              panel=FALSE\n                              )\nplot(linearity_check, data=price_mileage_model)\n\n$NCV\n\n\n\n\n\nOne odd thing to note: you need to pass the fitted model object as the data arguemnt to the plot function, not the original data set.\n\n\n1.3.2 Checking Homogeneity of Variance with a Fitted vs. Residuals plot\nThe only thing that changes for producing a a Fitted vs. Residuals plot designed to check for homoskedasticity (a fancy word for “equal variance”) is writing check=\"homogeneity\" in the check_model() function:\n\nvariance_check <- check_model(price_mileage_model, check=\"homogeneity\",\n                              panel=FALSE\n                              )\nplot(variance_check, data=price_mileage_model)\n\n$HOMOGENEITY\n\n\n\n\n\n\n\n1.3.3 Checking Normality with a Normal-Quantile plot\n\nnormality_check <- check_model(price_mileage_model, check=\"qq\",\n                              panel=FALSE\n                              )\nplot(normality_check, data=price_mileage_model)\n\nFor confidence bands, please install `qqplotr`.\n\n\n$QQ"
  },
  {
    "objectID": "01_simple_linear_regression.html#transformationsreexpressions",
    "href": "01_simple_linear_regression.html#transformationsreexpressions",
    "title": "1  Simple Linear Regression",
    "section": "1.4 Transformations/Reexpressions",
    "text": "1.4 Transformations/Reexpressions\nSection 1.4 introduces a new data set and new model into the mix, to demonstrate how transformations of the outcome and/or explantory variable may be useful when the conditions for a simple linear regression model are not met.\nThe CountyHealth data set measures the number of doctors and the number of hospitals from 53 Counties in the United States.\n\ndata(CountyHealth)\nCountyHealth\n\n\n\n  \n\n\n\nExample 1.7 explores a simple linear model which casts the number doctors in the county as a function of how many hospitals are in the county. However, when we fit this model, we see that not all the conditions for inference seem reasonable\n\ndoctor_model <- lm(MDs ~ Hospitals, data = CountyHealth)\nextract_eq(doctor_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{MDs}} = -1120.56 + 557.32(\\operatorname{Hospitals})\n\\]\n\n\n\nggplot(CountyHealth, aes(x=Hospitals, y= MDs)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, formula=y~x) +\n  scale_y_continuous(breaks=seq(0,8000,by=2000))\n\n\n\n\n\ncheck_model(doctor_model, check=c(\"homogeneity\", \"qq\"))\n\n\n\n\nThe residual errors around this model’s predictions grow larger and more variable as the number of hospitals increases, and they don’t follow a Normal distribution. But, modeling the square root of the number of doctor’s alleviates these problems.\nThere are several ways you can adjust to modeling the square root ofa variable. One method is to make a new variable in the data set that holds the transformed values, following the process we did in Section 1.1.7 (where we used the mutate() function to help center the Mileage variable).\nAnother method is to apply the transformation within the model formula itself! We can apply the sqrt() function to the MDs variable at the same time fit the model:\n\nsqrt_doctor_model <- lm(sqrt(MDs) ~ Hospitals, data = CountyHealth)\ncoef(sqrt_doctor_model)\n\n(Intercept)   Hospitals \n  -2.753326    6.876364 \n\n\nOne advantage of doing the transformation directly in the model formula is that the extract_eq() function is able to detect the transformation, and include it in the equation!\n\nextract_eq(sqrt_doctor_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{sqrt(MDs)}} = -2.75 + 6.88(\\operatorname{Hospitals})\n\\]\n\n\nThe diagnostics plots based on the model using the \\(\\sqrt{MDs}\\) variable indicate that the Normality assumption is met, and the Equal Variance is much more tenable on the transformed scale:\n\nsqrt_doctor_model <- lm(sqrt(MDs) ~ Hospitals, data = CountyHealth)\ncheck_model(sqrt_doctor_model, check=c(\"homogeneity\", \"qq\"))\n\n\n\n\n\n1.4.1 Visualizing Transformed Models\nOften times when an assumption is violated, we may decide to fit the model on a transformed scale, but visualize the model’s predictions on the original scale (since the original scale is more interpretable than say, the square-root scale).\nThe easiest way to do this when plotting with ggplot and geom_smooth() is to use a square root scale transformation, following by a coordinate transformation that squares the y-axis values (thus reversing the square root operation). What makes this work is that the scale transformation occurs first, so geom_smooth() fits and draws the model on the transformed (and linear!) scale, and coordinate transformation follows, so the model’s predictions are presented on the “raw” scale.\n\nggplot(CountyHealth, aes(x=Hospitals, y= MDs)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, formula=y~x) +\n  scale_y_sqrt(breaks=seq(0,8000,by=2000), expand=c(0,10)) +\n  coord_trans(y = scales::trans_new(\"square\", function(x) x^2, \"sqrt\"))\n\n\n\n\nAnother approach is to manually compute a grid of predictions for each x-axis position using the fitted model equation, then square and plot each predicted value. Though not demonstrated with transformations specifically, this type of approach is demonstrated in Chapter 2, when plotting prediction intervals.\n\n\n1.4.2 Log-Transformed Outcome Variables\nAll the previous techniques for modeling and visualizing the square-root of an outcome variable applicable to modeling with variables that have been transformed with the logarithmic function as well: we just use the log() function instead of the sqrt() function!\nAs an example, we can reproduce the fitted model, predictions and visualizations from Example 1.8, where the logarithm of the number of mammal species from 13 islands in Southeast Asia is modeled using the logarithm of the Area of the island as an explanatory variable. Since both variables have been transformed, this type of model is know as a “log-log model”.\n\ndata(\"SpeciesArea\")\nSpeciesArea\n\n\n\n  \n\n\n\n\nlog_species_model <- lm(log(Species) ~ log(Area), data = SpeciesArea)\nextract_eq(log_species_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{log(Species)}} = 1.62 + 0.24(\\operatorname{\\log(Area)})\n\\]\n\n\n\nggplot(SpeciesArea, aes(x = Area, y = Species)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, formula=y~x) +\n  scale_x_continuous(trans = \"log\") +\n  scale_y_continuous(trans = \"log\")\n\n\n\n\nReproduces Figure 1.19(b), but uses axis labels on the raw instead of logarithmic scale\n\n\n\n\n\n\n\n\nAnderson, Daniel, Andrew Heiss, and Jay Sumners. 2022. Equatiomatic: Transform Models into ’LaTeX’ Equations. https://CRAN.R-project.org/package=equatiomatic.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Dominique Makowski. 2021. “performance: An R Package for Assessment, Comparison and Testing of Statistical Models.” Journal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html",
    "href": "02_inference_for_simple_linear_regression.html",
    "title": "2  Inference for Simple Linear Regression",
    "section": "",
    "text": "Chapter 2 uses the same simple linear regression model from Chapter 1 (the mode that uses mileage to explain the price of a used Honda Accord) as example to explain the logic of inferences based on Null Hypothesis Significance Tests (NHST) and the resulting p-values.\nLuckily for us, we don’t have to learn many new tricks to execute the same hypothesis tests reported in Chapter 2, because the content of these tests were already present in the summaries we learned how to create in the previous chapter! However, we’ll reproduce those tables here, and point out where you can find the relevant statistics seen in Chapter 2 in the output seen from R."
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#inference-for-regression-slope",
    "href": "02_inference_for_simple_linear_regression.html#inference-for-regression-slope",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.1 Inference for Regression Slope",
    "text": "2.1 Inference for Regression Slope\n\n2.1.1 A t-test for the slope coefficient\nChapter 2.1 demonstrates that the t-statistic for the slope coefficient can be found by dividing the slope coefficient’s value by it’s estimated standard error. In this example, the t-statistic for the Mileage slope was shown to be -8.5. We can find the same standard error and t-statistic in the regression table produced by calling the summary() function on the fitted model object:\n\n\n\n\nlibrary(Stat2Data)\ndata(\"AccordPrice\")\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.8096     0.9529   21.84  < 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,    Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\nLooking in the “Coefficients” section of the out, The standard error values for the intercept and slope are found in the column labeled Std. Error, and the t-statistic values are found in the adjacent column labeled t value.\nIn the last column of this regression table are the p-values associated with each t-statistic. Since the p-values for the intercept and slope coefficient in this model very small numbers, R displays their value in scientific notation. You can tell R is using scientific notation by the presence of the lower case e in the value, followed by a negative integer.\nFor example, the p value shown for the Mileage slope’s t-statistic is 3.06e-09. This notation means “move the decimal place to the left by 9 places to find the precise value”. So 3.06e-09 in scientific notation translates to an actual p-value of 0.00000000306 - a very small number indeed! It’s easy to misread the p-values given in scientific notation as very large numbers instead of very small numbers if you are quickly glancing over the table, so be sure to read them carefully!\n\n\n2.1.2 A Confidence Interval for the slope coefficient\nOne piece of information about the slope coefficient the is noticeably absent from the regression table is the 95% confidence interval. Example 2.1 demonstrates how to find the bounds of the 95% confidence interval by applying the formula:\n\\[\n\\beta_1 \\pm t^* \\cdot SE_{\\beta_1}\n\\]\nwhere \\(t^*\\) is the value of the 97.5th percentile of the \\(t_{n-2}\\) distribution. \\(\\beta_1\\) and \\(SE_{\\beta_1}\\) are easily found in the regression table, but finding the value of \\(t^*\\) will require one more computation. We can find this “critical value” by using the qt() function:\n\n\n\n\ncrit_t <- qt(p = .975, df=30-2)\ncrit_t\n\n[1] 2.048407\n\n\nThe p argument reflects the fact that we’re interested in the 97.5th percentile (expressed as the proportion .975, instead of a percentage). And, we need to supply the appropriate degrees of freedom for this t-distribution, which in this case is 28 (30 cars gives us 30 degrees to freedom to begin with, minus two for the intercept and slope coefficients estimated while fitting the model).\nNow, we have the “ingredients” for our confidence interval formula:\n\nbeta_1 <- -0.1198\ncrit_t <- qt(p = .975, df=30-2)\nSE_beta <- 0.0141\n\nlower <- beta_1 +- crit_t * SE_beta\nupper <- beta_1 + crit_t * SE_beta\nc(\"Lower\" = lower, \"Upper\" = upper)\n\n      Lower       Upper \n-0.14868254 -0.09091746 \n\n\nLuckily, we don’t have to take the time and effort to implement this formula manually; there are several high-level ways to perform this computation more quickly (and with less rounding error!). The confint() function is one such method:\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nconfint(price_mileage_model)\n\n                 2.5 %      97.5 %\n(Intercept) 18.8577657 22.76146004\nMileage     -0.1486848 -0.09093915\n\n\nThe default setting for the confint() function is to produce a 95% confidence interval, but you can customize the confidence level by providing a different proportion as the level argument:\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\nconfint(price_mileage_model, level = .99)\n\n                 0.5 %      99.5 %\n(Intercept) 18.1766079 23.44261776\nMileage     -0.1587608 -0.08086308\n\n\nAs useful as the confint() function is, it’s often a bit awkward to have your regression table separated from the confidence interval for your coefficient. A useful function that can produce the regression table including the confidence interval boundaries is the tidy function from the broom package (Robinson, Hayes, and Couch 2022)\n\nlibrary(broom)\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\ntidy(price_mileage_model, conf.int = TRUE, conf.level = .99)\n\n\n\n  \n\n\n\nThis regression table has all the same information as the one produced by the summary(), just with slightly different names:\n\nCorrespondence between columns in the summary() regression table and the broom::tidy() regression table\n\n\nbroom::tidy() regression table\nsummary() regression table\n\n\n\n\nterm\nrow names\n\n\nestimate\nEstimate\n\n\nstd.error\nStd. Error\n\n\nstatistic\nt value\n\n\np.value\nPr(>|t|)\n\n\nconf.low\nNo corresponding column\n\n\nconf.high\nNo corresponding column"
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#partitioning-variability---anova",
    "href": "02_inference_for_simple_linear_regression.html#partitioning-variability---anova",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.2 Partitioning Variability - ANOVA",
    "text": "2.2 Partitioning Variability - ANOVA\nAlthough the ANOVA table isn’t explained until Chapter 2, we already saw how to produce it for ourselves back in Chapter 1 using R’s anova() function.\nHowever, there is one discrepancy between the output shown in the textbook, and R’s anova() function: R does not display an “SS Total” row in it’s ANOVA table. This is a minor loss, since the “SS Total” is of course, based on the sum of all the previous rows.\nHowever, if you do need that row for some particular reason, it is easily reproduced with a little help from dplyr:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\noriginal_table <- as_tibble(anova(price_mileage_model), rownames = \"Term\")\n\ntotal_row <- tibble(Term = \"Total\",\n                    Df = sum(original_table$Df),\n                    `Sum Sq` = sum(original_table$`Sum Sq`)\n                    ) %>%\n  mutate(`Mean Sq` = `Sum Sq`/Df)\n\nfull_table <- bind_rows(original_table, total_row)\nfull_table"
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#regression-and-correlation",
    "href": "02_inference_for_simple_linear_regression.html#regression-and-correlation",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.3 Regression and Correlation",
    "text": "2.3 Regression and Correlation\n\n2.3.1 The Coefficient of Determination \\(R^2\\)\nOnce again, we don’t need to learn how to do any new computations to find the \\(R^2\\) value of a linear model: it’s already shown in the output from R’s summary() command:\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.8096     0.9529   21.84  < 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,  Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\n\n\n\n2.3.2 The Correlation Coefficient\nThe correlation coefficient between two variables can be computed using R’s cor() function, supplying either the outcome explanatory variable as the x argument, and the remaining argument as the y argument. Here, we use cor() as a summary function inside of dplyr’s summarize() function:\n\nAccordPrice |>\n  summarize(r = cor(x = Mileage, y = Price))\n\n\n\n  \n\n\n\nTo perform a t-test on the correlation coefficient, you can use the cor.test() function. The syntax for using the cor.test() closely resembles the syntax for the lm() function, using a formula and a data argument. However, since the correlation coefficient is symmetric (and neither variable is considered the ‘outcome’ or ‘explanatory’ variable), both variables go on the right hand side of the tilde, separated by a +.\n\ncor.test(formula = ~ Mileage + Price, data=AccordPrice)\n\n\n    Pearson's product-moment correlation\n\ndata:  Mileage and Price\nt = -8.5002, df = 28, p-value = 3.055e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9259982 -0.7039888\nsample estimates:\n       cor \n-0.8489441"
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#intervals-for-predictions",
    "href": "02_inference_for_simple_linear_regression.html#intervals-for-predictions",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.4 Intervals for Predictions",
    "text": "2.4 Intervals for Predictions\nIn Section 2.4, two intervals around the regression line are introduced:\n\nThe confidence interval around the conditional mean of the outcome variable\nThe “prediction interval” around the conditional value of the outcome variable.\n\nThe formulas given for these intervals are similar to that for the confidence interval around the value of the slope coefficient, only with a more complex standard error estimator. Rather than implement these formulas ourselves, we’ll rely on R’s predict() function to get the job done.\nR’s predict() function relies on using a fitted model object to generate predictions about the outcome variable. For example, if you give the predict() function our fitted Accord price model, it will return the predicted price of each car in the original data set, based on the known Mileage value and the fitted regression equation that relates Mileage to Price.\n\nprice_mileage_model <- lm(Price ~ Mileage, data = AccordPrice)\npredict(price_mileage_model)\n\n        1         2         3         4         5         6         7         8 \n11.835698 14.459580 11.332488 14.807034 13.381272 20.234515 10.098425 18.317524 \n        9        10        11        12        13        14        15        16 \n20.234515 15.022696 15.238357 20.450177 13.129667 19.815174 17.562709 18.377430 \n       17        18        19        20        21        22        23        24 \n12.614476 10.397955 13.081742  2.777915 12.997874 14.088163  4.107827 19.144227 \n       25        26        27        28        29        30 \n18.581111 18.928565 16.196853 18.437336  6.516047  6.132649 \n\n\nThe predict() function isn’t limited to generating predictions for Mileage values in the original data set - we can generate prediction for any Mileage value we choose by including a new data frame of Mileage values as the newdata argument to the predict() function:\n\nnew_mileages <- data.frame(Mileage = c(20, 50, 100))\npredict(price_mileage_model, newdata = new_mileages)\n\n        1         2         3 \n18.413374 14.819015  8.828418 \n\n\nAnd by changing the interval argument to either \"confidence\" or \"prediction\", we can find the upper and lower boundaries of the 95% confidence interval or prediction interval respectively:\n\nnew_mileages <- data.frame(Mileage = c(20, 50, 100))\n# 95% confidence interval around conditional mean of y\npredict(price_mileage_model, newdata = new_mileages, interval = \"confidence\")\n\n        fit       lwr      upr\n1 18.413374 16.888598 19.93815\n2 14.819015 13.657874 15.98016\n3  8.828418  7.080566 10.57627\n\n# 95% Prediction interval around conditional value of y\npredict(price_mileage_model, newdata = new_mileages, interval = \"prediction\")\n\n        fit       lwr      upr\n1 18.413374 11.912606 24.91414\n2 14.819015  8.393807 21.24422\n3  8.828418  2.271739 15.38510\n\n\n\n2.4.1 Visualizing Intervals Around a Regression Model\nIn Chapter 1, we learned how to visualize the predictions of a regression model using ggplot() and the geom_smooth() function specifically. In that example, we included the argument se = FALSE to suppress the confidence interval band around the regression line, since this interval had not been introduced yet.\nThis means that drawing the 95% confidence interval around your model’s predictions is as easy as removing se = FALSE from your code!\n\nlibrary(ggplot2)\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x)\n\n\n\n\nDrawing the 95% prediction interval is a bit more involved, since geom_smooth() doesn’t include the ability to draw them auto-magically.\nTo draw the boundaries of this interval as a smooth line across the entire range of the plot, we’ll first need to compute the upper and lower boundaries of the interval across a fine grid of x-axis values. Here, we’ll use the seq() function to create a sequence of Mileage values spaced out by .1 miles, starting at 0 and ending at 150:\n\nnew_mileages <- data.frame(Mileage = seq(from = 0, to = 150, by = .1))\n\nThen, we’ll give these new Mileage values to the predict() function, and ask for boundaries of the 95% prediction interval around each of these nrow(new_mileages) Mileage values:\n\nprediction_interval <- predict(price_mileage_model,\n                               newdata = new_mileages,\n                               interval = \"prediction\", \n                               )\n\nSince we need to eventually supply these values to ggplot, and ggplot only deals with data frames, we’ll need to coerce these values from a matrix into a data frame:\n\nprediction_interval <- as.data.frame(prediction_interval)\n\nLastly, we need to combine these predictions about the outcome variable with the Mileage values we based on them on. Each row of the new_mileage data frame represents a Mileage value, and each row of the predcition_interval data frame represents the corresponding predicted Price. So, what we need to is combine these two data frames together “side by side”, so each row matches up! Let’s use the bind_cols() function from the dplyr() package to do this:\n\nprediction_interval <- bind_cols(new_mileages, prediction_interval)\n\nNow, we’re ready to plot! Here, we demonstrate two ways of adding this interval to the plot. First, we’ll draw the boundaries of this interval as two separate dashed lines:\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x) +\n  geom_line(data = prediction_interval, \n            mapping = aes(y=upr),\n            color=\"red\", linetype=2\n            ) +\n  geom_line(data = prediction_interval, \n            mapping = aes(y=lwr),\n            color=\"red\", linetype=2\n            )\n\n\n\n\nNote that we had to supply the prediction_interval data frame as a layer-specific data frame for the geom_line function.\nIf you prefer the “filled in” style of interval (like how the confidence interval appears from geom_smooth()), you can use geom_ribbon():\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x) +\n  geom_ribbon(data = prediction_interval, \n              mapping = aes(y=fit, ymax=upr, ymin=lwr),\n              color=\"red\", fill=\"red\",\n              alpha=.05, linetype=2\n              )\n\n\n\n\nTake note of a few important things about this version of the prediction interval:\n\nWe drew it before drawing the confidence interval and the regression line, so the shading of the confidence interval would not be affected by the shading of the prediction interval.\nThe names of the y, ymax and ymin aesthetic matched the names of the columns in the prediction_interval data set\nWe set the alpha argument to a very small number, in order to make the interval transparent, and avoid obscuring the data points. We recommend that you use an alpha value between .02 and .1 for your plots.\n\n\n\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom."
  },
  {
    "objectID": "03_multiple_regression.html",
    "href": "03_multiple_regression.html",
    "title": "3  Multiple Regression",
    "section": "",
    "text": "The first multiple regression model introduced in Chapter 3 models the winning percentage of the 32 NFL teams during the 2016 regular season as a function of each teams total points scored on offense, and their total points allowed on defense.\nThese WinPct(the outcome variable), PointsFor, and PointsAgainst (the two explanatory variables) are found in the NFLStandings2016 data set, which comes with the Stat2Data R package:\nBefore introducing the regression model that uses all three variables, the authors explore bivariate relationships between different combinations these variables. To this end, the authors present a scatter plot matrix in Figure 3.1, which holds scatter plots based on each possible two-variable combination that could be made out of these three variables.\nThe easiest way to produce a scatter plot matrix for ourselves is to use the ggpairs() function from the GGally R package (Schloerke et al. 2021). By default, the ggpairs() function will build the scatter plot matrix using all the variables in the data set provided to it. Since we only wish to use three of the variables, we’ll use the select() function from the dplyr package to create a new data set holding just the WinPct, PointsFor, and PointsAgainst variables, and base our plot on this smaller data set:\nggpairs() produces a scatter plot matrix with a few differences compared to the one in the book: it displays a density plot of each individual variable along the diagonal of the matrix (instead of the variable names), and prints the correlation coefficient in the upper triangle of the matrix (instead of the same scatter plots from the lower triangle with their axis transposed). The default ggpairs() scatter plot does display more information, but if you wish to exactly reproduce the Figure 3.1, you can customize what is displayed on the diagonal and upper triangle:\nFigure 3.2 focuses in on the top-middle and top-right panel of the scatter plot matrix, which show the win percentage vs. points scored relationship, and the win percentage vs. points allowed relationships, respectively. This figure places the two scatter plots side-by-side, adds a visualization of the bivariate regression model to each panel, and annotates the plot with the value of the \\(R^2\\) statistic.\nWe learned in Chapter 1 how to add a visualization of the bivariate regression model to a scatter plot, but we’ve yet to learn 1) how to place two separate scatter plots side-by-side, and 2) how to annotate the plot with text.\nLet’s learn how to annotate the plot with the \\(R^2\\) statistic first. Of course, before we annotate the plot with the \\(R^2\\) statistic, we’ve got to compute the \\(R^2\\) statistic! So, let’s fit the two bivariate models, and extract the \\(R^2\\) statistic from each one:\nNext, we’ll annotate the WinPct vs. PointsFor scatter plot with the \\(R^2\\) statistic for that model, using the annotate() function from the ggplot2() package:\nIn order to prepare for juxtaposing both annotated scatter plots, we’ll save this plot as a variable. Notice that when we do this, no plot is printed out!\nNext, we repeat this same basic process with the WinPct ~ PointsAgainst model and scatter plot, changing the variable names and x/y position values where appropriate:\nFinally, we’ll take advantage of the grid.arrange() function from the gridExtra package to place the two plots side-by-side:"
  },
  {
    "objectID": "03_multiple_regression.html#multiple-linear-regression-model",
    "href": "03_multiple_regression.html#multiple-linear-regression-model",
    "title": "3  Multiple Regression",
    "section": "3.1 Multiple Linear Regression Model",
    "text": "3.1 Multiple Linear Regression Model\nSection 3.1, Example 3.2 introduces the book’s first multiple regression model, using both the PointsFor and PointsAgainst variables to predict the WinPct value for each team. We can reproduce the regression table and model equation shown in this example using some familiar tools: the lm() function, the extract_eq() function, and the summary() function\nThe only place where we’ll notice a change in our code when we move from a “simple” linear model with one explanatory variable to a multiple regression with two explanatory variables is with our model formula inside the lm() function\n\nwinPct_multiple_reg <- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nTo add a second explanatory variable to the model, you literally add another explanatory variable to the model formula using the + sign, and that’s all there is to it! Once the model is fit, you can work with it in R just like it was a “simple” linear regression model:\n\nsummary(winPct_multiple_reg)\n\n\nCall:\nlm(formula = WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149898 -0.073482 -0.006821  0.072569  0.213189 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.7853698  0.1537422   5.108 1.88e-05 ***\nPointsFor      0.0016992  0.0002628   6.466 4.48e-07 ***\nPointsAgainst -0.0024816  0.0003204  -7.744 1.54e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09653 on 29 degrees of freedom\nMultiple R-squared:  0.7824,    Adjusted R-squared:  0.7674 \nF-statistic: 52.13 on 2 and 29 DF,  p-value: 2.495e-10\n\nlibrary(equatiomatic)\nextract_eq(winPct_multiple_reg, use_coefs=TRUE, coef_digits = 4)\n\n\\[\n\\operatorname{\\widehat{WinPct}} = 0.7854 + 0.0017(\\operatorname{PointsFor}) - 0.0025(\\operatorname{PointsAgainst})\n\\]\n\n\nOne thing we did have to do differently here was include the coef_digits argument fo the extract_eq() function. Because a single point has such a small effect on the win percentage for an entire season, the coefficients of this model as quite small values. Without setting the coef_digits argument` to a larger number, the equation would just show the coefficients rounded down to 0!\n\n3.1.1 Visualizing a Multiple Regression Model\nOne thing suspiciously absent from Section 3.1 is a visualization of the WinPct ~ PointsFor + PointsAgainst. Visualization of a model’s predictions can be a great aid in understanding the model’s structure, so why is such a figure absent?\nThe answer likely lies in the fact that visualizing a multiple regression model with two numeric explanatory variables requires a 3D plot instead of a 2D plot. Indeed, we need a third dimension to measure our third numeric variable along! So, this visualization omission can be forgiven if we are willing to admit that 3D plots in a 2D book are well, a bit tricky.\nEven though 3 dimensions are more complex than 2, it’s still not too hard to lean how to do in R. One of easier ways to construct 3D plots showing the predictions of a multiple with two numeric explanatory variables, along with the data the model was fit to, is with the regplanely package (the reason for the “plane” in the name will become clear soon!).\nBecause the regplanely package is an “unofficial” R package (i.e., it’s not included in the Comprehensive R Archive Network that you normally install packages from), we’ll have to install it a different way. First, install the devtools package by running this command in your R console:\n\ninstall.packages(\"devtools\")\n\nNext, use the install_github function to install the regplanely package straight from the location of it’s source code archive, on github.\n\ndevtools::install_github(\"wjhopper/regplanely\")\n\nWith the regplanely package installed, we can finally put it to use! The regression_plane() function takes a fitted regression model object, and uses it to draw the predictions of the regression model as a plane in 3D space.\n\nlibrary(regplanely)\n\nwinPct_multiple_reg <- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\nregression_plane(winPct_multiple_reg)\n\n\n\n\n\nSince the regression_plane() function builds it’s visualizations with plotly instead of ggplot2, you can interact with these plots (e.g., zoom, rotate, etc.) in RStudio or a web browser."
  },
  {
    "objectID": "03_multiple_regression.html#assessing-a-multiple-regression-model",
    "href": "03_multiple_regression.html#assessing-a-multiple-regression-model",
    "title": "3  Multiple Regression",
    "section": "3.2 Assessing a Multiple Regression Model",
    "text": "3.2 Assessing a Multiple Regression Model\nSection 3.2 is more focused on inferential statistics in the context of multiple regression, but as we know, it’s unwise to focus on inferential statistics without first examining the pre-conditions (the linearity, equal variance, and Normality assumptions).\nFortunately, it’s just as easy to examine these assumptions for a multiple regression model with several numeric explanatory variables as it is for a ‘simple’ linear regression model. We can use the exact same tools from the performance() package we used back in Chapter 1.\nIn example with the Honda Accord prices, we examined the linearity, equal variance, and Normality assumption with three separate plots (two Fitted vs. Residuals plots, and one Normal Quantile plot). Since we’re more familiar with each individual plot, now, let’s save a bit of time and ask the performance() package to display them all each of them within a single visualization:\n\nlibrary(performance)\n\nwinPct_multiple_reg <- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\ncheck_model(winPct_multiple_reg,\n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"normality\")\n            )\n\n\n\n\nThe plots in the bottom row (the Normal quantile plot, and the Normal density plot) are two ways of assessing the same thing (the assumption that the residuals are Normally distributed). But, the Normal density plot complements the Normal quantile plot for a reader who might not be comfortable with a QQ plot (and who wants to leave 1/4 of their plot blank?).\n\n3.2.1 t-Tests for Coefficients\nThe t-tests (and the associated p-values) for each coefficient are in the\n\nwinPct_multiple_reg <- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\nsummary(winPct_multiple_reg)\n\n\nCall:\nlm(formula = WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149898 -0.073482 -0.006821  0.072569  0.213189 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.7853698  0.1537422   5.108 1.88e-05 ***\nPointsFor      0.0016992  0.0002628   6.466 4.48e-07 ***\nPointsAgainst -0.0024816  0.0003204  -7.744 1.54e-08 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09653 on 29 degrees of freedom\nMultiple R-squared:  0.7824,    Adjusted R-squared:  0.7674 \nF-statistic: 52.13 on 2 and 29 DF,  p-value: 2.495e-10\n\n\n\n\n\n3.2.2 Confidence Intervals for Coefficients\nConfidence intervals around the coefficients of a multiple linear regression model have the same mathematical form as for confidence intervals around the coefficients of a “simple” linear regression model. And, we can still use the tidy() function from the broom package to obtain a regression table supplemented with confidence intervals for each coefficient:\n\nlibrary(broom)\n\ntidy(winPct_multiple_reg, conf.int = TRUE)\n\n\n\n  \n\n\n\nNotice that the bounds for the coefficient of the PointsFor variable (shown in the conf.low and conf.high columns) match the bounds shown in Example 3.5"
  },
  {
    "objectID": "03_multiple_regression.html#anova-for-multiple-regression",
    "href": "03_multiple_regression.html#anova-for-multiple-regression",
    "title": "3  Multiple Regression",
    "section": "3.3 ANOVA for Multiple Regression",
    "text": "3.3 ANOVA for Multiple Regression\nThe ANOVA tables shown in Chapter 3 display 3 rows:\n\nThe degrees of freedom, sum of squares, mean square and F-statistic for the “Model” or “Regression” source of variance\nThe degrees of freedom, sum of squares, and mean square for the “Error” source of variance\nThe total degrees of freedom, sum of squares, and mean square for both the outcome variable\n\nHowever, R’s anova() function produces a slightly different ANOVA table, as shown below:\n\nwinPct_multiple_reg <- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\nanova(winPct_multiple_reg)\n\n\n\n  \n\n\n\nThe most important different is that R does not report a single row for the “Model” source of variance; instead, R reports the degrees of freedom, sum of squares, mean square and F-statistic for each explanatory variable in the model.\nEach row in the table before the “Residuals” row summarizes the variance in the outcome explained by each explanatory variable, after each previously summarized explanatory variable is taken into account. For example, the 0.55884 value for the sums of squares associated with the PointsAgainst variable represents the additional variability in team win percentage after the PointsFor variable is taken into account.\nNotice that the “Regression” row in the ANOVA table shown in Example 3.6 can be recovered by summing the PointsFor and PointsAgainst rows of R’s ANOVA table:\n\nanova(winPct_multiple_reg) |>\n  as_tibble() |>\n  slice(1:2) |> # keeps just the first two rows of the table \n  summarize(Df = sum(Df),\n            `Sum Sq` = sum(`Sum Sq`),\n            ) |>\n  mutate(`Mean Sq` = `Sum Sq`/Df)\n\n\n\n  \n\n\n\n\n3.3.1 Coefficient of Multiple Determination\nThe adjusted \\(R^2\\) statistic is introduced in Section 3.2. The adjusted \\(R^2\\) statistic is a penalized version of the “plain” \\(R^2\\) statistic, with the penalty growing with the number explanatory variables in the model.\nThere’s no need for you to implement the rather tricky computation for the adjusted \\(R^2\\) statistic: just like its “plain” counterpart, the adjusted \\(R^2\\) statistic can be found in the output from the summary() function, beneath the table of regression coefficients:\n\nsummary(winPct_multiple_reg)\n\n\nCall:\nlm(formula = WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149898 -0.073482 -0.006821  0.072569  0.213189 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.7853698  0.1537422   5.108 1.88e-05 ***\nPointsFor      0.0016992  0.0002628   6.466 4.48e-07 ***\nPointsAgainst -0.0024816  0.0003204  -7.744 1.54e-08 ***\n---\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09653 on 29 degrees of freedom\nMultiple R-squared:  0.7824,  Adjusted R-squared:  0.7674 \nF-statistic: 52.13 on 2 and 29 DF,  p-value: 2.495e-10\n\n\n\n\n\n3.3.2 Confidence and Prediction Intervals\nThe easiest way to compute confidence and prediction intervals around our model’s predicted outcome is to hand our fitted model object off to the predict() function. For example:\n\npredict(winPct_multiple_reg, interval = \"confidence\")\n\n         fit        lwr       upr\n1  0.9143024 0.82272694 1.0058778\n2  0.7413510 0.68174866 0.8009534\n3  0.6745702 0.62362296 0.7255175\n4  0.5368107 0.49004268 0.5835788\n5  0.6953926 0.59054904 0.8002362\n6  0.6073397 0.53716546 0.6775139\n7  0.6518565 0.60556587 0.6981472\n8  0.6622498 0.60297051 0.7215291\n9  0.5565525 0.50359855 0.6095063\n10 0.4591635 0.42279419 0.4955328\n11 0.6141597 0.55507867 0.6732408\n12 0.4848726 0.44832699 0.5214181\n13 0.4454766 0.38264895 0.5083042\n14 0.4711684 0.43570558 0.5066313\n15 0.4947114 0.45755194 0.5318709\n16 0.5077908 0.46698762 0.5485940\n17 0.5715934 0.52495111 0.6182357\n18 0.5109439 0.46410266 0.5577852\n19 0.5791490 0.52370313 0.6345949\n20 0.5972853 0.55202918 0.6425414\n21 0.5252962 0.48466189 0.5659304\n22 0.4556371 0.36627654 0.5449976\n23 0.5875573 0.54635178 0.6287629\n24 0.5558981 0.50347493 0.6083213\n25 0.4147637 0.37193665 0.4575908\n26 0.2376723 0.17199363 0.3033509\n27 0.4323159 0.37453506 0.4900968\n28 0.1882391 0.10327672 0.2732015\n29 0.2692847 0.20772914 0.3308402\n30 0.3330701 0.28452810 0.3816120\n31 0.1192516 0.03129992 0.2072032\n32 0.1122738 0.02697032 0.1975773\n\n\nreturns the conditional mean win percentage, upper confidence interval bound on the mean win percentage, and lower confidence interval bound on the mean win percentage for each observation in the NFLStandings2016 data set.\nAnd we can still generate the confidence interval boundaries on the mean win percentage for arbitrary values of our explanatory variables by supplying a new data frame of “observed” values to the precict() function. But, constructing this new data frame of values can be a bit more involved when dealing with a multiple regression model: now, we need to create a data frame with multiple columns in it, because our model uses combination of explanatory variables to generate it’s predictions.\nOne function that is quite useful in this context is the expand.grid() function1. You supply the expand.grid() function one or more vector of values, and it constructs a data frame whose rows hold all possible combinations of the values from that variable. This is quite useful for generating a grid of values at which to evaluate the predictions of your multiple regression model!\nFor example, consider the example below, which constructs a data frame whose 9 rows hold all 9 possible combinations of 200, 300, and 400 points allowed, and 200, 300, and 400 points scored:\n\npoints_grid <- expand.grid(PointsFor = c(200, 300, 400),\n                           PointsAgainst = c(200, 300, 400)\n                           )\npoints_grid\n\n\n\n  \n\n\n\nWe can pass this grid of combinations to the predict() function to find the 99% prediction interval at each location:\n\npredict(winPct_multiple_reg, newdata = points_grid,\n        interval = \"prediction\", level = .99\n        )\n\n        fit         lwr       upr\n1 0.6288852  0.29856134 0.9592091\n2 0.7988006  0.48799331 1.1096078\n3 0.9687159  0.66117342 1.2762584\n4 0.3807276  0.07947089 0.6819843\n5 0.5506429  0.27037295 0.8309129\n6 0.7205583  0.44335641 0.9977601\n7 0.1325700 -0.16407923 0.4292192\n8 0.3024853  0.02661435 0.5783563\n9 0.4724006  0.19908257 0.7457187\n\n\nAt the time of writing2, there is straightforward way to visualize the confidence interval or prediction interval around the model using the regression_plane() function from the regplanely package. So for the time being, your confidence intervals around the conditional means will have to live in a table, rather that in a visualization.\n\n\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ’Ggplot2’. https://CRAN.R-project.org/package=GGally."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, Daniel, Andrew Heiss, and Jay Sumners. 2022. Equatiomatic:\nTransform Models into ’LaTeX’ Equations. https://CRAN.R-project.org/package=equatiomatic.\n\n\nCannon, A. R., G. W. Cobb, B. A. Hartlaub, J. M. Legler, R. H. Lock, T.\nL. Moore, A. J. Rossman, and J. A. Witmer. 2018. Stat2: Modeling\nwith Regression and ANOVA. Macmillan Learning. https://www.macmillanlearning.com/college/us/product/STAT2/p/1319054072.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip\nWaggoner, and Dominique Makowski. 2021. “performance: An R Package for\nAssessment, Comparison and Testing of Statistical Models.”\nJournal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. Broom: Convert\nStatistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz\nMarbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally:\nExtension to ’Ggplot2’. https://CRAN.R-project.org/package=GGally.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\nDplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  }
]