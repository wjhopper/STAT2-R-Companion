[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The R Companion to STATS2",
    "section": "",
    "text": "Preface\nThis is a guide to conducting the analysis demonstrated in Stat2: Modeling with Regression and ANOVA (Cannon et al. 2018) using R, with a focus on using tools and techniques from the Tidyverse.\nThis guide assumes some prior familiarity with R programming, so that it may focus on demonstrating the content from STAT2, rather than teaching R wholesale. Thankfully, there are many free resources introducing R programming and tidyverse packages the reader may take advantage of to gain this familiarity. Below are a few books which provide a mostly comprehensive introduction:\n\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse\nData Science: A First Introduction\nYaRrr! The Pirate’s Guide to R (focused more on ‘base’ R than the tidyverse, but still good.)\nHands-On Programming with R\n\n\n\n\n\nCannon, A. R., G. W. Cobb, B. A. Hartlaub, J. M. Legler, R. H. Lock, T. L. Moore, A. J. Rossman, and J. A. Witmer. 2018. STAT2: Modeling with Regression and ANOVA. Macmillan Learning. https://www.macmillanlearning.com/college/us/product/STAT2/p/1319054072."
  },
  {
    "objectID": "01_simple_linear_regression.html#the-simple-linear-regression-model",
    "href": "01_simple_linear_regression.html#the-simple-linear-regression-model",
    "title": "1  Simple Linear Regression",
    "section": "1.1 The Simple Linear Regression Model",
    "text": "1.1 The Simple Linear Regression Model\nLinear regression is introduced in Chapter 1 with the motivating question:\n\nHow much should you expect to pay for a used Honda Accord, if you know how many miles the car has already been driven?\n\nand introduces the AccordPrice data set, which contains information about the list price and mileage for a sample of 30 Honda Accords. The AccordPrice data set is included with the Stat2Data R package, so to access the data for yourself, you’ll need to install the package. If you don’t already know how to install R packages, here are two good resources to walk you through the process:\n\nReading: ModernDive Chapter 1.3.1: Installing Packages\nWatching: How to Install Packages in R Studio on YouTube\n\nOnce you have the package installed, load the package into your R session using:\n\nlibrary(Stat2Data)\n\nTo load the AccordPrice data set into your R environment, use the command:\n\ndata(\"AccordPrice\")\n\n\n\n\n\n  \n\n\n\nAs a side note: not much information is given in the text about how this sample of 30 Accords was collected, but we can gather a bit more information by looking at the help page for the AccordPrice data set. To open the help page for the AccordPrice data set, you can run the command\n\n?AccordPrice\n\nin the R console. By reading the “Details” and “Source” sections, we can learn that these 30 Accords were listed for sale on Cars.com in Lakewood Ohio during February 2017. Whenever you want to to know more about one of the textbook’s data sets, the help page for that data set is a good place to look first. Sometimes there’s not much more information than given in the textbook, but every little bit helps!\n\n1.1.1 Exploring the AccordPrice data set\nFigure 1.2 displays a scatter plot of the Mileage and Price variable, showing how those variables relate to one another. To re-produce this scatter plot, we’ll use the ggplot2 R package (Wickham 2016). If you’re not already familiar with the ggplot2 package, here are a few good resources to help you get started:\n\nReading: ModernDive Chapter 2: Data Visualization\nReading: Effective data visualization\nWatching: ggplot for plots and graphs on YouTube\n\nTo re-create this scatter plot, we’ll map the Mileage variable to x-axis aesthetic, and the Price variable to the y-axis aesthetic, and draw a layer of points to represent each of the 30 cars using geom_point()\n\nlibrary(ggplot2)\n\nggplot(data = AccordPrice,\n       mapping = aes(x=Mileage, y=Price)\n       ) +\n  geom_point()\n\n\n\n\n\nIf you want to exactly reproduce the scatter plots in STAT2, right down to the colors, backgrounds, and fonts, you can use the following ggplot2 theme:\n\n\nCode\ntheme_stat2 &lt;- function(base_size = 11,\n                        base_family = \"\",\n                        base_line_size = base_size/22,\n                        base_rect_size = base_size/22) {\n  \n  theme_bw() %+replace% \n  theme(axis.text.x = element_text(color=\"black\"),\n        axis.text.y = element_text(color=\"black\"),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background=element_rect(colour=\"black\"),\n        complete = TRUE\n        )\n}\n\n\nAnd use the hex color code #92278f for your geometric objects. For example, this could exactly reproduce Figure 1.2 by adapting the code above to use this new theme:\n\n\nCode\nggplot(data = AccordPrice,\n       mapping = aes(x=Mileage, y=Price)\n       ) +\n  geom_point(color=\"#92278f\") +\n  theme_stat2()\n\n\nIn the rest of this book, we won’t use the STAT2 theme for our visualizations, but provide it here for completeness.\n\n\n\n1.1.2 Modeling the Mileage vs. Price relationsip\nExample 1.3 shows a summary of a simple linear regression model fit to the Mileage and Price variable in the AccordPrices data set. This summary is actually a mix of two different summaries, a regression table and an Analysis of Variance (ANOVA) table. Reproducing this summary will be a 3 step process in R:\n\nFitting the model using the lm() function\nPrinting the regression table with the summary() function\nPrinting the ANOVA table with the anova() function\n\n\n\n1.1.3 Fitting a simple linear regression model\nThe lm() function (short for linear model) does the “heavy lifting’ of estimating the coefficients of the simple linear model. In other words, the lm() function fits the model to the observed data by finding the optimal values for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) in the model \\(Price = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot Mileage + \\epsilon\\).\nTo fit a linear regression model using lm, you need to supply:\n\nA formula describing relationship between the outcome and explanatory variable(s)\nThe name of a data set where the outcome and explanatory variables can be found.\n\nIn this case, our call to the lm function would be:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\n\nThe first argument inside the lm() function is the formula describing the structure of the model. In R, model formulas are always created using the ~ symbol, with the outcome variable named on the left, and the explanatory variables(s) named on the right. As you might notice, R’s model formula code is an adaptation of how the model is described in mathematical notation.\nAlso, take note that we’ve saved the results from fitting this linear model in a new R object named price_mileage_model. We’ll need to use this new object to produce the regression table and the ANOVA table in steps 2 and 3 below.\n\n\n1.1.4 Reporting the regression table\nIn order to report the regression table, we need to call the summary() function on the linear model object we just created:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.8096     0.9529   21.84  &lt; 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,    Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\nAs we can see, the summary() function first prints out a few things not shown as part of the summary in the textbook: a copy of the code used to fit the model, and a the Five-number summary of the model’s residual errors. These are followed by the regression table summarizing the intercept and slope, and a “goodness of fit” summary of the model as whole.\n\n\n1.1.5 Reporting the ANOVA table\nThe ANOVA table is found by calling the aptly named anova() function on the linear model, the same way we just did with the summary() function a moment ago:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nanova(price_mileage_model)\n\n\n\nAnalysis of Variance Table\n\nResponse: Price\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nMileage    1 687.66  687.66  72.253 3.055e-09 ***\nResiduals 28 266.49    9.52                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n1.1.6 Adding the regression line to a scatterplot\nFigure 1.3 shows the Price vs. Mileage scatter plot again, but this time with a line representing the regression model’s predictions drawn on top of the raw data. Surprisingly, the easiest method for visualizing the predictions of a regression model doesn’t involve the fitted model object. Instead, we will begin with the same ggplot code we used to draw the Mileage vs. Price scatter plot earlier, and add to it. The geom_smooth() function is used to draw the regression line on top of the raw data:\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE, formula = y~x)\n\n\n\n\ngeom_smooth() is a generic smoothing function: the key argument that tells it to fit and display a linear regression model is the method = lm argument. Without the method=lm argument, geom_smooth() will not display a linear model.\nThe se = FALSE argument is included to stop ggplot from drawing confidence interval bands around the regression line. And, the formula = y~x argument is included simply to prevent ggplot from printing an annoying message that says geom_smooth() using formula ‘y ~ x’ when creating the plot.\n\n\n1.1.7 Residual Errors\nAn important component of any statistical model is the residual error - the difference between the observed value of the outcome variable, and the value your model predicted that outcome to be, based on the same explanatory variable value. No model will predict every observation in your data literally perfectly, and measuring the kinds of errors your model makes is crucial for knowing whether you’ve built a reasonable model of you data.\nThe residual errors will feature prominently when we assess the conditions for inference later in this chapter. For now, we’ll just learn how to compute the residual errors around your model predictions, but we won’t do anything more with those values.\nThe simplest way to compute the difference between observed outcome and predicted outcome for each observation in your data set is to use the residuals() function on your fitted model object:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nresiduals(price_mileage_model)\n\n         1          2          3          4          5          6          7 \n 0.1643021  3.4404204  4.3675123 -2.3070342 -3.8812720  1.2654845 -6.5984246 \n         8          9         10         11         12         13         14 \n 4.4824757  6.5654845 -1.4226957  4.1616428 -0.9501770 -4.1296669 -2.4151737 \n        15         16         17         18         19         20         21 \n 0.2372910 -0.8774303  0.8855244 -3.3979545 -1.4817422  5.1220854 -1.2978738 \n        22         23         24         25         26         27         28 \n 1.5118375  0.8921728  1.8557732 -2.9811106 -1.9285653 -0.1968528 -0.8373363 \n        29         30 \n 0.3839526 -0.6326491 \n\n\nThis returns a vector with as many elements as there are rows in your data set, and each element in the vector measures the residual error between model and data for that particular row. For example, the residual error for the Accord price in the first row of the data set was 0.1643021, meaning the model over-predicted it’s price by 0.1643021 thousand dollars (or, 164 dollars).\nBut, one shortcoming of this function is that the residual errors are separated from the observed outcome values and the predicted outcome values. If you only need the residuals this is not an issue, but more often than the not, you’ll need context to help understand and use the residuals effectively.\nThe augment() function from the broom() package produces a table that contains the original data the model was fit to, the predicted value for each observation, and the residual error for that observation:\n\nlibrary(broom)\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\naugment(price_mileage_model)\n\n\n\n  \n\n\n\nThe predicted values are in the .fitted columns, and the residual errors are in the .resid column. The output holds many other pieces of information which will become useful in the future, but for now we can ignore the .hat, .sigma, .cooksd and .std.resid columns.\n\n\n1.1.8 Centering the Mileage Variable\nExample 1.4 demonstrates how centering a variable (i.e., shifting all the values left or right by a single chosen number) changes the interpretation of the intercept coefficient, but not the slope coefficient. In this example, the Mileage variable is shifted to the left by 50; in other words, 50 is subtracted from all the Mileage values before fitting the model.\nThe easiest way to replicate this model is create a new variable in the AccordPrices data set which holds the centered Mileage values. To make this new column, we’ll use the mutate function from the dplyr package (Wickham et al. 2022). If you aren’t familiar with the mutate() function or the dplyr package, here are a few good resources to investigate:\n\nReading: ModernDive Chapter 3: Data Wrangling\nReading: Cleaning and Wrangling Data\nWatching: Dplyr Essentials on YouTube\n\nIn this case, the ‘mutation’ we apply is quite simple: we just use the subtraction operator to subtract 50, and R automatically applies this subtraction to all 30 values in the Mileage column.\n\nlibrary(dplyr)\n\nAccordPrice &lt;- AccordPrice |&gt;\n  mutate(Mileage_c50 = Mileage - 50)\nAccordPrice\n\n\n\n  \n\n\n\nNote that we saved our centered mileage scores in a variable named Mileage_c50, to help us keep track of what these values mean: they are mileage values that have been centered by 50.\nFrom here, we just need to fit another linear model with lm(), using our new Mileage_c50 variable as the explanatory variable in our model formula:\n\ncentered_mileage_model &lt;- lm(Price ~ Mileage_c50, data = AccordPrice)\n\nThe textbook only presents the fitted model equation (not the full regression table) in order to show the intercept and slope coefficients. If you ever need just the coefficient values, without the rest of the summaries in the regression table, you can use the coef() function on your model object to print them out:\n\ncentered_mileage_model &lt;- lm(Price ~ Mileage_c50, data = AccordPrice)\ncoef(centered_mileage_model)\n\n(Intercept) Mileage_c50 \n 14.8190154  -0.1198119 \n\n\n\n\n1.1.9 Displaying the fitted model equation\nIf you are using a literate programming environment (like an RMarkdown or Quarto document), you might find yourself wanting to display the fitted model equation in your document, formatted like a “fancy” mathematical equation. You could always write the LaTeX markup you need yourself, but the equatiomatic package (Anderson, Heiss, and Sumners 2022) can automatically generate what you need, straight from the model object itself!\nTo demonstrate, let’s display a formatted equation representing the fitted regression model based on the centered mileage scores by using the extract_eq() function on the model object.\n\nlibrary(equatiomatic)\ncentered_mileage_model &lt;- lm(Price ~ Mileage_c50, data = AccordPrice)\nextract_eq(centered_mileage_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{Price}} = 14.82 - 0.12(\\operatorname{Mileage\\_c50})\n\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs the time of writing, there are problems with using the equatiomatic package to display equations when rendering Quarto documents to PDF. Thankfully, there is a workaround that is not too difficult, which involves saving the equation as a variable, and cat()-ing the equation yourself:\n```{r}\n#| results: asis\neq &lt;- extract_eq(centered_mileage_model, use_coefs = TRUE)\ncat(\"$$\", eq, \"$$\", sep = \"\\n\")\n```\n\\[\n\\operatorname{\\widehat{Price}} = 14.82 - 0.12(\\operatorname{Mileage\\_c50})\n\\]\nJust be sure to set the results: asis chunk option!"
  },
  {
    "objectID": "01_simple_linear_regression.html#conditions-for-a-simple-linear-model",
    "href": "01_simple_linear_regression.html#conditions-for-a-simple-linear-model",
    "title": "1  Simple Linear Regression",
    "section": "1.2 Conditions for a Simple Linear Model",
    "text": "1.2 Conditions for a Simple Linear Model\nSection 1.2 introduces mostly conceptual information about necessary and sufficient conditions for inference on a linear model, but does introduce the formula for estimating the standard error of the regression (also called the “Residual Standard Error”). You’ll rarely need to use this formula “manually”, since the value of this statistic is included in the output from the summary() function:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.8096     0.9529   21.84  &lt; 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,    Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\n\nIf you ever need to obtain this value alone (without the rest of the summary table), you can use the sigma() function on the fitted model object to extract it;\n\nsigma(price_mileage_model)\n\n[1] 3.08504"
  },
  {
    "objectID": "01_simple_linear_regression.html#assessing-conditions",
    "href": "01_simple_linear_regression.html#assessing-conditions",
    "title": "1  Simple Linear Regression",
    "section": "1.3 Assessing Conditions",
    "text": "1.3 Assessing Conditions\nSection 1.4 introduces two type of plots that crucial for assessing the validity of the assumptions underpinning theory-based inference on a regression model:\n\nThe Fitted vs. Residuals Plot\nThe Normal Quantile plot (which is a specific type of Quantile-Quantile plot)\n\nThere are many ways of generating these plots from a linear model in R, but perhaps the easiest, most full-featured (and prettiest!) method is to use the tools from the performance package (Lüdecke et al. 2021).\nThe check_model() function from the performance package can be used to create both fitted vs. residuals plots, and Normal Quantile plots. What’s more, it can create different variations of fitted vs. residuals plots that are customized to help you check either the Linearity or the Homogeneity of Variance assumptions.\n\n\n\n\n\n\nTip\n\n\n\nYou’ll also want to install the patchwork and see packages at the same time you install the performance package, as these supplemental packages are very useful for constructing visualizations with the performance package.\n\n\n\n1.3.1 Checking Linearity with a Fitted vs. Residuals plot\nTo generate a variant of the Fitted vs. Residuals plot designed to help you assess the Linearity assumption, you’ll include the check=\"linearity\" argument to the check_model() function. The panel=FALSE argument instructs the check_model() function to devote the entire plot window to this plot (instead of leaving room for other plots checking other model assumptions).\n\nlibrary(performance)\nlibrary(see)\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nlinearity_check &lt;- check_model(price_mileage_model, check=\"linearity\",\n                              panel=FALSE\n                              )\nplot(linearity_check, data=price_mileage_model)\n\n$NCV\n\n\n\n\n\nOne odd thing to note: you need to pass the fitted model object as the data argument to the plot function, not the original data set.\n\n\n1.3.2 Checking Homogeneity of Variance with a Fitted vs. Residuals plot\nThe only thing that changes for producing a a Fitted vs. Residuals plot designed to check for homoskedasticity (a fancy word for “equal variance”) is writing check=\"homogeneity\" in the check_model() function:\n\nvariance_check &lt;- check_model(price_mileage_model, check=\"homogeneity\",\n                              panel=FALSE\n                              )\nplot(variance_check, data=price_mileage_model)\n\n$HOMOGENEITY\n\n\n\n\n\n\n\n1.3.3 Checking Normality with a Normal-Quantile plot\n\nnormality_check &lt;- check_model(price_mileage_model, check=\"qq\",\n                              panel=FALSE\n                              )\nplot(normality_check, data=price_mileage_model)\n\nFor confidence bands, please install `qqplotr`.\n\n\n$QQ"
  },
  {
    "objectID": "01_simple_linear_regression.html#transformationsreexpressions",
    "href": "01_simple_linear_regression.html#transformationsreexpressions",
    "title": "1  Simple Linear Regression",
    "section": "1.4 Transformations/Reexpressions",
    "text": "1.4 Transformations/Reexpressions\nSection 1.4 introduces a new data set and new model into the mix, to demonstrate how transformations of the outcome and/or explanatory variable may be useful when the conditions for a simple linear regression model are not met.\nThe CountyHealth data set measures the number of doctors and the number of hospitals from 53 Counties in the United States.\n\ndata(CountyHealth)\nCountyHealth\n\n\n\n  \n\n\n\nExample 1.7 explores a simple linear model which casts the number doctors in the county as a function of how many hospitals are in the county. However, when we fit this model, we see that not all the conditions for inference seem reasonable\n\ndoctor_model &lt;- lm(MDs ~ Hospitals, data = CountyHealth)\nextract_eq(doctor_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{MDs}} = -1120.56 + 557.32(\\operatorname{Hospitals})\n\\]\n\n\n\nggplot(CountyHealth, aes(x=Hospitals, y= MDs)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, formula=y~x) +\n  scale_y_continuous(breaks=seq(0,8000,by=2000))\n\n\n\n\n\ncheck_model(doctor_model, check=c(\"homogeneity\", \"qq\"))\n\n\n\n\nThe residual errors around this model’s predictions grow larger and more variable as the number of hospitals increases, and they don’t follow a Normal distribution. But, modeling the square root of the number of doctor’s alleviates these problems.\nThere are several ways you can adjust to modeling the square root of a variable. One method is to make a new variable in the data set that holds the transformed values, following the process we did in Section 1.1.8 (where we used the mutate() function to help center the Mileage variable).\nAnother method is to apply the transformation within the model formula itself! We can apply the sqrt() function to the MDs variable at the same time fit the model:\n\nsqrt_doctor_model &lt;- lm(sqrt(MDs) ~ Hospitals, data = CountyHealth)\ncoef(sqrt_doctor_model)\n\n(Intercept)   Hospitals \n  -2.753326    6.876364 \n\n\nOne advantage of doing the transformation directly in the model formula is that the extract_eq() function is able to detect the transformation, and include it in the equation!\n\nextract_eq(sqrt_doctor_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{sqrt(MDs)}} = -2.75 + 6.88(\\operatorname{Hospitals})\n\\]\n\n\nThe diagnostics plots based on the model using the \\(\\sqrt{MDs}\\) variable indicate that the Normality assumption is met, and the Equal Variance is much more tenable on the transformed scale:\n\nsqrt_doctor_model &lt;- lm(sqrt(MDs) ~ Hospitals, data = CountyHealth)\ncheck_model(sqrt_doctor_model, check=c(\"homogeneity\", \"qq\"))\n\n\n\n\n\n1.4.1 Visualizing Transformed Models\nOften times when an assumption is violated, we may decide to fit the model on a transformed scale, but visualize the model’s predictions on the original scale (since the original scale is more interpretable than say, the square-root scale).\nThe easiest way to do this when plotting with ggplot and geom_smooth() is to use a square root scale transformation, following by a coordinate transformation that squares the y-axis values (thus reversing the square root operation). What makes this work is that the scale transformation occurs first, so geom_smooth() fits and draws the model on the transformed (and linear!) scale, and coordinate transformation follows, so the model’s predictions are presented on the “raw” scale.\n\nggplot(CountyHealth, aes(x=Hospitals, y= MDs)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, formula=y~x) +\n  scale_y_sqrt(breaks=seq(0,8000,by=2000), expand=c(0,10)) +\n  coord_trans(y = scales::trans_new(\"square\", function(x) x^2, \"sqrt\"))\n\n\n\n\nAnother approach is to manually compute a grid of predictions for each x-axis position using the fitted model equation, then square and plot each predicted value. Though not demonstrated with transformations specifically, this type of approach is demonstrated in Chapter 2, when plotting prediction intervals.\n\n\n1.4.2 Log-Transformed Outcome Variables\nAll the previous techniques for modeling and visualizing the square-root of an outcome variable applicable to modeling with variables that have been transformed with the logarithmic function as well: we just use the log() function instead of the sqrt() function!\nAs an example, we can reproduce the fitted model, predictions and visualizations from Example 1.8, where the logarithm of the number of mammal species from 13 islands in Southeast Asia is modeled using the logarithm of the Area of the island as an explanatory variable. Since both variables have been transformed, this type of model is know as a “log-log model”.\n\ndata(\"SpeciesArea\")\nSpeciesArea\n\n\n\n  \n\n\n\n\nlog_species_model &lt;- lm(log(Species) ~ log(Area), data = SpeciesArea)\nextract_eq(log_species_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{log(Species)}} = 1.62 + 0.24(\\operatorname{\\log(Area)})\n\\]\n\n\n\nggplot(SpeciesArea, aes(x = Area, y = Species)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, formula=y~x) +\n  scale_x_continuous(trans = \"log\") +\n  scale_y_continuous(trans = \"log\")\n\n\n\n\nReproduces Figure 1.19(b), but uses axis labels on the raw instead of logarithmic scale\n\n\n\n\n\n\n\n\nAnderson, Daniel, Andrew Heiss, and Jay Sumners. 2022. Equatiomatic: Transform Models into ’LaTeX’ Equations. https://CRAN.R-project.org/package=equatiomatic.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Dominique Makowski. 2021. “performance: An R Package for Assessment, Comparison and Testing of Statistical Models.” Journal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#inference-for-regression-slope",
    "href": "02_inference_for_simple_linear_regression.html#inference-for-regression-slope",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.1 Inference for Regression Slope",
    "text": "2.1 Inference for Regression Slope\n\n2.1.1 A t-test for the slope coefficient\nChapter 2.1 demonstrates that the t-statistic for the slope coefficient can be found by dividing the slope coefficient’s value by it’s estimated standard error. In this example, the t-statistic for the Mileage slope was shown to be -8.5. We can find the same standard error and t-statistic in the regression table produced by calling the summary() function on the fitted model object:\n\nlibrary(Stat2Data)\ndata(\"AccordPrice\")\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.8096     0.9529   21.84  &lt; 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,    Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\nLooking in the “Coefficients” section of the out, The standard error values for the intercept and slope are found in the column labeled Std. Error, and the t-statistic values are found in the adjacent column labeled t value.\nIn the last column of this regression table are the p-values associated with each t-statistic. Since the p-values for the intercept and slope coefficient in this model very small numbers, R displays their value in scientific notation. You can tell R is using scientific notation by the presence of the lower case e in the value, followed by a negative integer.\nFor example, the p value shown for the Mileage slope’s t-statistic is 3.06e-09. This notation means “move the decimal place to the left by 9 places to find the precise value”. So 3.06e-09 in scientific notation translates to an actual p-value of 0.00000000306 - a very small number indeed! It’s easy to misread the p-values given in scientific notation as very large numbers instead of very small numbers if you are quickly glancing over the table, so be sure to read them carefully!\n\n\n2.1.2 A Confidence Interval for the slope coefficient\nOne piece of information about the slope coefficient the is noticeably absent from the regression table is the 95% confidence interval. Example 2.1 demonstrates how to find the bounds of the 95% confidence interval by applying the formula:\n\\[\n\\beta_1 \\pm t^* \\cdot SE_{\\beta_1}\n\\]\nwhere \\(t^*\\) is the value of the 97.5th percentile of the \\(t_{n-2}\\) distribution. \\(\\beta_1\\) and \\(SE_{\\beta_1}\\) are easily found in the regression table, but finding the value of \\(t^*\\) will require one more computation. We can find this “critical value” by using the qt() function:\n\ncrit_t &lt;- qt(p = .975, df=30-2)\ncrit_t\n\n[1] 2.048407\n\n\nThe p argument reflects the fact that we’re interested in the 97.5th percentile (expressed as the proportion .975, instead of a percentage). And, we need to supply the appropriate degrees of freedom for this t-distribution, which in this case is 28 (30 cars gives us 30 degrees to freedom to begin with, minus two for the intercept and slope coefficients estimated while fitting the model).\nNow, we have the “ingredients” for our confidence interval formula:\n\nbeta_1 &lt;- -0.1198\ncrit_t &lt;- qt(p = .975, df=30-2)\nSE_beta &lt;- 0.0141\n\nlower &lt;- beta_1 +- crit_t * SE_beta\nupper &lt;- beta_1 + crit_t * SE_beta\nc(\"Lower\" = lower, \"Upper\" = upper)\n\n      Lower       Upper \n-0.14868254 -0.09091746 \n\n\nLuckily, we don’t have to take the time and effort to implement this formula manually; there are several high-level ways to perform this computation more quickly (and with less rounding error!). The confint() function is one such method:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nconfint(price_mileage_model)\n\n                 2.5 %      97.5 %\n(Intercept) 18.8577657 22.76146004\nMileage     -0.1486848 -0.09093915\n\n\nThe default setting for the confint() function is to produce a 95% confidence interval, but you can customize the confidence level by providing a different proportion as the level argument:\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\nconfint(price_mileage_model, level = .99)\n\n                 0.5 %      99.5 %\n(Intercept) 18.1766079 23.44261776\nMileage     -0.1587608 -0.08086308\n\n\nAs useful as the confint() function is, it’s often a bit awkward to have your regression table separated from the confidence interval for your coefficient. A useful function that can produce the regression table including the confidence interval boundaries is the tidy function from the broom package (Robinson, Hayes, and Couch 2022)\n\nlibrary(broom)\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\ntidy(price_mileage_model, conf.int = TRUE, conf.level = .99)\n\n\n\n  \n\n\n\nThis regression table has all the same information as the one produced by the summary(), just with slightly different names:\n\nCorrespondence between columns in the summary() regression table and the broom::tidy() regression table\n\n\nbroom::tidy() regression table\nsummary() regression table\n\n\n\n\nterm\nrow names\n\n\nestimate\nEstimate\n\n\nstd.error\nStd. Error\n\n\nstatistic\nt value\n\n\np.value\nPr(&gt;|t|)\n\n\nconf.low\nNo corresponding column\n\n\nconf.high\nNo corresponding column"
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#partitioning-variability---anova",
    "href": "02_inference_for_simple_linear_regression.html#partitioning-variability---anova",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.2 Partitioning Variability - ANOVA",
    "text": "2.2 Partitioning Variability - ANOVA\nAlthough the ANOVA table isn’t explained until Chapter 2, we already saw how to produce it for ourselves back in Chapter 1 using R’s anova() function.\nHowever, there is one discrepancy between the output shown in the textbook, and R’s anova() function: R does not display an “SS Total” row in it’s ANOVA table. This is a minor loss, since the “SS Total” is of course, based on the sum of all the previous rows.\nHowever, if you do need that row for some particular reason, it is easily reproduced with a little help from dplyr:\n\nlibrary(dplyr)\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\noriginal_table &lt;- as_tibble(anova(price_mileage_model), rownames = \"Term\")\n\ntotal_row &lt;- tibble(Term = \"Total\",\n                    Df = sum(original_table$Df),\n                    `Sum Sq` = sum(original_table$`Sum Sq`)\n                    ) %&gt;%\n  mutate(`Mean Sq` = `Sum Sq`/Df)\n\nfull_table &lt;- bind_rows(original_table, total_row)\nfull_table"
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#regression-and-correlation",
    "href": "02_inference_for_simple_linear_regression.html#regression-and-correlation",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.3 Regression and Correlation",
    "text": "2.3 Regression and Correlation\n\n2.3.1 The Coefficient of Determination \\(R^2\\)\nOnce again, we don’t need to learn how to do any new computations to find the \\(R^2\\) value of a linear model: it’s already shown in the output from R’s summary() command:\n\nsummary(price_mileage_model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = AccordPrice)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5984 -1.8169 -0.4148  1.4502  6.5655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.8096     0.9529   21.84  &lt; 2e-16 ***\nMileage      -0.1198     0.0141   -8.50 3.06e-09 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.085 on 28 degrees of freedom\nMultiple R-squared:  0.7207,  Adjusted R-squared:  0.7107 \nF-statistic: 72.25 on 1 and 28 DF,  p-value: 3.055e-09\n\n\n\n\n\n2.3.2 The Correlation Coefficient\nThe correlation coefficient between two variables can be computed using R’s cor() function, supplying either the outcome explanatory variable as the x argument, and the remaining argument as the y argument. Here, we use cor() as a summary function inside of dplyr’s summarize() function:\n\nAccordPrice |&gt;\n  summarize(r = cor(x = Mileage, y = Price))\n\n\n\n  \n\n\n\nTo perform a t-test on the correlation coefficient, you can use the cor.test() function. The syntax for using the cor.test() closely resembles the syntax for the lm() function, using a formula and a data argument. However, since the correlation coefficient is symmetric (and neither variable is considered the ‘outcome’ or ‘explanatory’ variable), both variables go on the right hand side of the tilde, separated by a +.\n\ncor.test(formula = ~ Mileage + Price, data=AccordPrice)\n\n\n    Pearson's product-moment correlation\n\ndata:  Mileage and Price\nt = -8.5002, df = 28, p-value = 3.055e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9259982 -0.7039888\nsample estimates:\n       cor \n-0.8489441"
  },
  {
    "objectID": "02_inference_for_simple_linear_regression.html#intervals-for-predictions",
    "href": "02_inference_for_simple_linear_regression.html#intervals-for-predictions",
    "title": "2  Inference for Simple Linear Regression",
    "section": "2.4 Intervals for Predictions",
    "text": "2.4 Intervals for Predictions\nIn Section 2.4, two intervals around the regression line are introduced:\n\nThe confidence interval around the conditional mean of the outcome variable\nThe “prediction interval” around the conditional value of the outcome variable.\n\nThe formulas given for these intervals are similar to that for the confidence interval around the value of the slope coefficient, only with a more complex standard error estimator. There are several ways to compute these intervals in R without implementing the formula’s yourself; The “classic” way to compute these intervals to use the predict() function (which is included with base R). But, since our ultimate goal will be visualizing these intervals around our regression line, using the augment() function from the broom package will be a better choice in the long run.\nAs we learned when computing residual errors in Chapter 1, the augment() function uses the fitted model object to compute it’s predictions. But by default, the augment() function doesn’t compute any confidence or predictions intervals; To obtain these intervals, we’ll need to include the interval argument. For example, including interval=\"confidence\" will instruct augment() to include the upper and lower boundaries of the 95% confidence interval around the predicted outcome value for each point in the original data set:\n\nlibrary(broom) # for the augment() function\nlibrary(dplyr) # for the select() function\n\nprice_mileage_model &lt;- lm(Price ~ Mileage, data = AccordPrice)\naugment(price_mileage_model, interval=\"confidence\")\n\n\n\n  \n\n\n\nThe upper bound of the 95% confidence interval is in the .upper column, and the lower bound of the interval is in the .lower column. To obtain, say, a 99% confidence interval, we simply specify the confidence level as a proportion using the conf.level argument:\n\naugment(price_mileage_model, interval=\"confidence\", conf.level=.99)\n\n\n\n  \n\n\n\nTo obtain a prediction interval instead of a confidence interval, simply change the interval argument to \"prediction\" instead of \"confidence\"\n\naugment(price_mileage_model, interval=\"prediction\", conf.level=.99)\n\n\n\n  \n\n\n\n\n2.4.1 Computing intervals around new observations\nThe agument() function isn’t limited to generating confidence and prediction intervals based on the Mileage values in the original data set - we can generate intervals for any Mileage values we choose! All we have to do is include a new data frame of Mileage values as the newdata argument to the augment() function:\n\nnew_mileages &lt;- data.frame(Mileage = c(20, 50, 100))\naugment(price_mileage_model, newdata = new_mileages, interval = \"confidence\")\n\n\n\n  \n\n\n\n\n\n2.4.2 Visualizing Intervals Around a Regression Model\nIn Chapter 1, we learned how to visualize the predictions of a regression model using ggplot() and the geom_smooth() function specifically. In that example, we included the argument se = FALSE to suppress the confidence interval band around the regression line, since this interval had not been introduced yet.\nThis means that drawing the 95% confidence interval around your model’s predictions is as easy as removing se = FALSE from your code!\n\nlibrary(ggplot2)\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x)\n\n\n\n\nHowever, geom_smooth() doesn’t have to option to draw prediction intervals instead of confidence; so if you want one, you’ll have to plot it “manually”: by computing the boundary values yourself using augment() and drawing those values on the plot using geom_line() or geom_ribbon()\nSince we want the boundaries of the prediction interval to appear as a smooth curve across the entire range of the plot, we’ll first need to compute the upper and lower boundaries of the interval across a fine grid of x-axis values. So, we’ll need to create a series of many closely space mileage values, ranging from the minimum and maximum mileage values in the original data, and supply them to the augment() function as the newdata argument.\nCreating this series of closely spaced mileage values is the perfect job for the seq() function! Here, we’ll use it to create a sequence of Mileage values spaced out by .1 miles, starting at 0 and ending at 150:\n\nnew_mileages &lt;- data.frame(Mileage = seq(from = 0, to = 150, by = .1))\n\nThen, we’ll give these new Mileage values to the agument() function, and ask for boundaries of the 95% prediction interval around each of these nrow(new_mileages) Mileage values:\n\nprediction_interval &lt;- augment(price_mileage_model,\n                               newdata = new_mileages,\n                               interval = \"prediction\", \n                               )\n\nSince we need to eventually supply these values to ggplot, and ggplot only deals with data frames, we’ll need to coerce these values from a matrix into a data frame:\nNow, we’re ready to plot! Here, we demonstrate two ways of adding this interval to the plot. First, we’ll draw the boundaries of this interval as two separate dashed lines:\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x) +\n  geom_line(data = prediction_interval, \n            mapping = aes(y = .upper),\n            color = \"red\", linetype = 2\n            ) +\n  geom_line(data = prediction_interval, \n            mapping = aes(y = .lower),\n            color = \"red\", linetype = 2\n            )\n\n\n\n\nNote that we had to supply the prediction_interval data frame as a layer-specific data frame for the geom_line() function.\nIf you prefer the “filled in” style of interval (like how the confidence interval appears from geom_smooth()), you can use geom_ribbon() to plot the interval:\n\nggplot(data = AccordPrice,\n       mapping = aes(x = Mileage, y = Price)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x) +\n  geom_ribbon(data = prediction_interval, \n              mapping = aes(y = .fitted, ymax = .upper, ymin = .lower),\n              color = \"red\", fill = \"red\",\n              alpha = .05, linetype = 2\n              )\n\n\n\n\nTake note of a few important things about this version of the prediction interval:\n\nWe drew it before drawing the confidence interval and the regression line, so the shading of the confidence interval would not be affected by the shading of the prediction interval.\nThe names of the y, ymax and ymin aesthetic matched the names of the columns in the prediction_interval data set\nWe set the alpha argument to a very small number, in order to make the interval transparent, and avoid obscuring the data points. We recommend that you use an alpha value between .02 and .1 for your plots.\n\n\n\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom."
  },
  {
    "objectID": "03_multiple_regression.html#multiple-linear-regression-model",
    "href": "03_multiple_regression.html#multiple-linear-regression-model",
    "title": "3  Multiple Regression",
    "section": "3.1 Multiple Linear Regression Model",
    "text": "3.1 Multiple Linear Regression Model\nSection 3.1, Example 3.2 introduces the book’s first multiple regression model, using both the PointsFor and PointsAgainst variables to predict the WinPct value for each team. We can reproduce the regression table and model equation shown in this example using some familiar tools: the lm() function, the extract_eq() function, and the summary() function\nThe only place where we’ll notice a change in our code when we move from a “simple” linear model with one explanatory variable to a multiple regression with two explanatory variables is with our model formula inside the lm() function\n\nwinPct_multiple_reg &lt;- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nTo add a second explanatory variable to the model, you literally add another explanatory variable to the model formula using the + sign, and that’s all there is to it! Once the model is fit, you can work with it in R just like it was a “simple” linear regression model:\n\nsummary(winPct_multiple_reg)\n\n\nCall:\nlm(formula = WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149898 -0.073482 -0.006821  0.072569  0.213189 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.7853698  0.1537422   5.108 1.88e-05 ***\nPointsFor      0.0016992  0.0002628   6.466 4.48e-07 ***\nPointsAgainst -0.0024816  0.0003204  -7.744 1.54e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09653 on 29 degrees of freedom\nMultiple R-squared:  0.7824,    Adjusted R-squared:  0.7674 \nF-statistic: 52.13 on 2 and 29 DF,  p-value: 2.495e-10\n\nlibrary(equatiomatic)\nextract_eq(winPct_multiple_reg, use_coefs=TRUE, coef_digits = 4)\n\n\\[\n\\operatorname{\\widehat{WinPct}} = 0.7854 + 0.0017(\\operatorname{PointsFor}) - 0.0025(\\operatorname{PointsAgainst})\n\\]\n\n\nOne thing we did have to do differently here was include the coef_digits argument fo the extract_eq() function. Because a single point has such a small effect on the win percentage for an entire season, the coefficients of this model as quite small values. Without setting the coef_digits argument` to a larger number, the equation would just show the coefficients rounded down to 0!\n\n3.1.1 Visualizing a Multiple Regression Model\nOne thing suspiciously absent from Section 3.1 is a visualization of the WinPct ~ PointsFor + PointsAgainst. Visualization of a model’s predictions can be a great aid in understanding the model’s structure, so why is such a figure absent?\nThe answer likely lies in the fact that visualizing a multiple regression model with two numeric explanatory variables requires a 3D plot instead of a 2D plot. Indeed, we need a third dimension to measure our third numeric variable along! So, this visualization omission can be forgiven if we are willing to admit that 3D plots in a 2D book are well, a bit tricky.\nEven though 3 dimensions are more complex than 2, it’s still not too hard to lean how to do in R. One of easier ways to construct 3D plots showing the predictions of a multiple with two numeric explanatory variables, along with the data the model was fit to, is with the regplanely package (the reason for the “plane” in the name will become clear soon!).\nBecause the regplanely package is an “unofficial” R package (i.e., it’s not included in the Comprehensive R Archive Network that you normally install packages from), we’ll have to install it a different way. First, install the devtools package by running this command in your R console:\n\ninstall.packages(\"devtools\")\n\nNext, use the install_github function to install the regplanely package straight from the location of it’s source code archive, on github.\n\ndevtools::install_github(\"wjhopper/regplanely\")\n\nWith the regplanely package installed, we can finally put it to use! The regression_plane() function takes a fitted regression model object, and uses it to draw the predictions of the regression model as a plane in 3D space.\n\nlibrary(regplanely)\n\nwinPct_multiple_reg &lt;- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\nregression_plane(winPct_multiple_reg)\n\n\n\n\n\nSince the regression_plane() function builds it’s visualizations with plotly instead of ggplot2, you can interact with these plots (e.g., zoom, rotate, etc.) in RStudio or a web browser."
  },
  {
    "objectID": "03_multiple_regression.html#assessing-a-multiple-regression-model",
    "href": "03_multiple_regression.html#assessing-a-multiple-regression-model",
    "title": "3  Multiple Regression",
    "section": "3.2 Assessing a Multiple Regression Model",
    "text": "3.2 Assessing a Multiple Regression Model\nSection 3.2 is more focused on inferential statistics in the context of multiple regression, but as we know, it’s unwise to focus on inferential statistics without first examining the pre-conditions (the linearity, equal variance, and Normality assumptions).\nFortunately, it’s just as easy to examine these assumptions for a multiple regression model with several numeric explanatory variables as it is for a ‘simple’ linear regression model. We can use the exact same tools from the performance() package we used back in Chapter 1.\nIn example with the Honda Accord prices, we examined the linearity, equal variance, and Normality assumption with three separate plots (two Fitted vs. Residuals plots, and one Normal Quantile plot). Since we’re more familiar with each individual plot, now, let’s save a bit of time and ask the performance() package to display them all each of them within a single visualization:\n\nlibrary(performance)\n\nwinPct_multiple_reg &lt;- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\ncheck_model(winPct_multiple_reg,\n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"normality\")\n            )\n\n\n\n\nThe plots in the bottom row (the Normal quantile plot, and the Normal density plot) are two ways of assessing the same thing (the assumption that the residuals are Normally distributed). But, the Normal density plot complements the Normal quantile plot for a reader who might not be comfortable with a QQ plot (and who wants to leave 1/4 of their plot blank?).\n\n3.2.1 t-Tests for Coefficients\nThe t-tests (and the associated p-values) for each coefficient are in the\n\nwinPct_multiple_reg &lt;- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\nsummary(winPct_multiple_reg)\n\n\nCall:\nlm(formula = WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149898 -0.073482 -0.006821  0.072569  0.213189 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.7853698  0.1537422   5.108 1.88e-05 ***\nPointsFor      0.0016992  0.0002628   6.466 4.48e-07 ***\nPointsAgainst -0.0024816  0.0003204  -7.744 1.54e-08 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09653 on 29 degrees of freedom\nMultiple R-squared:  0.7824,    Adjusted R-squared:  0.7674 \nF-statistic: 52.13 on 2 and 29 DF,  p-value: 2.495e-10\n\n\n\n\n\n3.2.2 Confidence Intervals for Coefficients\nConfidence intervals around the coefficients of a multiple linear regression model have the same mathematical form as for confidence intervals around the coefficients of a “simple” linear regression model. And, we can still use the tidy() function from the broom package to obtain a regression table supplemented with confidence intervals for each coefficient:\n\nlibrary(broom)\n\ntidy(winPct_multiple_reg, conf.int = TRUE)\n\n\n\n  \n\n\n\nNotice that the bounds for the coefficient of the PointsFor variable (shown in the conf.low and conf.high columns) match the bounds shown in Example 3.5\n\n\n3.2.3 ANOVA for Multiple Regression\nThe ANOVA tables shown in Chapter 3 display 3 rows:\n\nThe degrees of freedom, sum of squares, mean square and F-statistic for the “Model” or “Regression” source of variance\nThe degrees of freedom, sum of squares, and mean square for the “Error” source of variance\nThe total degrees of freedom, sum of squares, and mean square for both the outcome variable\n\nHowever, R’s anova() function produces a slightly different ANOVA table, as shown below:\n\nwinPct_multiple_reg &lt;- lm(WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\nanova(winPct_multiple_reg)\n\n\n\n  \n\n\n\nThe most important different is that R does not report a single row for the “Model” source of variance; instead, R reports the degrees of freedom, sum of squares, mean square and F-statistic for each explanatory variable in the model.\nEach row in the table before the “Residuals” row summarizes the variance in the outcome explained by each explanatory variable, after each previously summarized explanatory variable is taken into account. For example, the 0.55884 value for the sums of squares associated with the PointsAgainst variable represents the additional variability in team win percentage after the PointsFor variable is taken into account.\nNotice that the “Regression” row in the ANOVA table shown in Example 3.6 can be recovered by summing the PointsFor and PointsAgainst rows of R’s ANOVA table:\n\nanova(winPct_multiple_reg) |&gt;\n  as_tibble() |&gt;\n  slice(1:2) |&gt; # keeps just the first two rows of the table \n  summarize(Df = sum(Df),\n            `Sum Sq` = sum(`Sum Sq`),\n            ) |&gt;\n  mutate(`Mean Sq` = `Sum Sq`/Df)\n\n\n\n  \n\n\n\n\n\n3.2.4 Coefficient of Multiple Determination\nThe adjusted \\(R^2\\) statistic is introduced in Section 3.2. The adjusted \\(R^2\\) statistic is a penalized version of the “plain” \\(R^2\\) statistic, with the penalty growing with the number explanatory variables in the model.\nThere’s no need for you to implement the rather tricky computation for the adjusted \\(R^2\\) statistic: just like its “plain” counterpart, the adjusted \\(R^2\\) statistic can be found in the output from the summary() function, beneath the table of regression coefficients:\n\nsummary(winPct_multiple_reg)\n\n\nCall:\nlm(formula = WinPct ~ PointsFor + PointsAgainst, data = NFLStandings2016)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149898 -0.073482 -0.006821  0.072569  0.213189 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.7853698  0.1537422   5.108 1.88e-05 ***\nPointsFor      0.0016992  0.0002628   6.466 4.48e-07 ***\nPointsAgainst -0.0024816  0.0003204  -7.744 1.54e-08 ***\n---\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09653 on 29 degrees of freedom\nMultiple R-squared:  0.7824,  Adjusted R-squared:  0.7674 \nF-statistic: 52.13 on 2 and 29 DF,  p-value: 2.495e-10\n\n\n\n\n\n3.2.5 Confidence and Prediction Intervals\nThe easiest way to compute confidence and prediction intervals around our model’s predicted outcome is to hand our fitted model object off to the predict() function. For example:\n\npredict(winPct_multiple_reg, interval = \"confidence\")\n\n         fit        lwr       upr\n1  0.9143024 0.82272694 1.0058778\n2  0.7413510 0.68174866 0.8009534\n3  0.6745702 0.62362296 0.7255175\n4  0.5368107 0.49004268 0.5835788\n5  0.6953926 0.59054904 0.8002362\n6  0.6073397 0.53716546 0.6775139\n7  0.6518565 0.60556587 0.6981472\n8  0.6622498 0.60297051 0.7215291\n9  0.5565525 0.50359855 0.6095063\n10 0.4591635 0.42279419 0.4955328\n [ reached getOption(\"max.print\") -- omitted 22 rows ]\n\n\nreturns the conditional mean win percentage, upper confidence interval bound on the mean win percentage, and lower confidence interval bound on the mean win percentage for each observation in the NFLStandings2016 data set.\nAnd we can still generate the confidence interval boundaries on the mean win percentage for arbitrary values of our explanatory variables by supplying a new data frame of “observed” values to the precict() function. But, constructing this new data frame of values can be a bit more involved when dealing with a multiple regression model: now, we need to create a data frame with multiple columns in it, because our model uses combination of explanatory variables to generate it’s predictions.\nOne function that is quite useful in this context is the expand.grid() function1. You supply the expand.grid() function one or more vector of values, and it constructs a data frame whose rows hold all possible combinations of the values from that variable. This is quite useful for generating a grid of values at which to evaluate the predictions of your multiple regression model!\nFor example, consider the example below, which constructs a data frame whose 9 rows hold all 9 possible combinations of 200, 300, and 400 points allowed, and 200, 300, and 400 points scored:\n\npoints_grid &lt;- expand.grid(PointsFor = c(200, 300, 400),\n                           PointsAgainst = c(200, 300, 400)\n                           )\npoints_grid\n\n\n\n  \n\n\n\nWe can pass this grid of combinations to the predict() function to find the 99% prediction interval at each location:\n\npredict(winPct_multiple_reg, newdata = points_grid,\n        interval = \"prediction\", level = .99\n        )\n\n        fit         lwr       upr\n1 0.6288852  0.29856134 0.9592091\n2 0.7988006  0.48799331 1.1096078\n3 0.9687159  0.66117342 1.2762584\n4 0.3807276  0.07947089 0.6819843\n5 0.5506429  0.27037295 0.8309129\n6 0.7205583  0.44335641 0.9977601\n7 0.1325700 -0.16407923 0.4292192\n8 0.3024853  0.02661435 0.5783563\n9 0.4724006  0.19908257 0.7457187\n\n\nAt the time of writing2, there is straightforward way to visualize the confidence interval or prediction interval around the model using the regression_plane() function from the regplanely package. So for the time being, your confidence intervals around the conditional means will have to live in a table, rather that in a visualization."
  },
  {
    "objectID": "03_multiple_regression.html#comparing-two-regression-lines",
    "href": "03_multiple_regression.html#comparing-two-regression-lines",
    "title": "3  Multiple Regression",
    "section": "3.3 Comparing Two Regression Lines",
    "text": "3.3 Comparing Two Regression Lines\nThe inauspicious name of Section 3.3 hides an important development: using a categorical variable as an explanatory variable in a regression model! This section introduces the reader to models with one numeric and one categorical explanatory variable. As the author’s point out, this development will allow us compare linear relationships across the groups determined by this categorical explanatory variable, and test whether aspects of the model (such as the slope, the intercept, or possibly both) differ reliably between groups.\nExample 3.9 uses the ButterfliesBc data set in order to introduce a parallel slopes regression model. This data set measures average wing length for male and female Boloria chariclea butterflies in Greenland, along with the average summer temperatures the butterflies were exposed to, for each year from 1996 through 2013.\n\nlibrary(Stat2Data)\ndata(\"ButterfliesBc\")\n\n\n\n\n\n  \n\n\n\nFigure 3.5 displays a scatter plot showing the relationship between the average wing length of the sampled butterflies, and the prior summer’s average temperature, with the “points” in the plot color-coded by the sex of the butterfly. Instead of providing a legend to map these colors back to the sex of the butterfly, the authors use the letters “m” and “f” as the “points” in this scatter plot. While this is a bit too clever (a legend is optimal), it’s worth recreating this figure because it teaches us a bit about ggplot:\nFirst, need a column in the data set that holds an “m” or an “f” in each row, depending on the sex of the butterfly\n\nButterfliesBc &lt;- ButterfliesBc |&gt;\n  mutate(Sex2 = recode(Sex, Male = \"m\", Female = \"f\")\n         )\n\nThen, we’ll map this new Sex2 variable to the label aesthetic of the plot, which will enable us to draw each observation on the plot using geom_text() (instead of geom_point()):\n\nggplot(data = ButterfliesBc,\n       mapping = aes(x = Temp, y = Wing, color = Sex, label = Sex2)) +\n  geom_text() +\n  guides(label = \"none\", color = \"none\")\n\n\n\n\nFigure 3.6 shows the same scatter plot, but with the predictions of the fitted parallel slopes model drawn on top of the observations. If you intuition is that we could easily create the same figure using the geom_smooth() function, your mind in this right place. However, mapping the color aesthetic to the Sex function means that within the internals of ggplot(), the data is grouped into separate sets (a male set and a female set), meaning geom_smooth() will fit a regression model to each sex separately. Fitting the model separately for each sex removes the constraint that there is one slope coefficient for both groups (fitting two separate models means estimating two slopes, by definition).\nWe could still reproduce Figure 3.6 by manually computing the predicted Wing lengths for a large number of closely-spaced temperature values (much like we did in Chapter 2, when constructing the bounds of the 95% prediction interval). But, there is also a slightly easier way: the geom_parallel_slopes() function from the moderndive R package (Kim, Ismay, and Kuhn 2021).\n\nlibrary(moderndive)\n\nggplot(data = ButterfliesBc,\n       mapping = aes(x = Temp, y = Wing, color = Sex, label = Sex2)) +\n  geom_text() +\n  guides(label = \"none\", color = \"none\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n\nNote that geom_parallel_slopes() does not need a method = lm argument: a parallel slopes model is a linear model by definition, so need to specify that with additional arguments!\nWe could always produce a more “traditional” version of this plot by dropping the label aesthetic, swapping geom_point() in place of geom_text(), and remove the guides() function (which suppressed ggplot’s automatic legend drawing):\n\nlibrary(moderndive)\n\nggplot(data = ButterfliesBc,\n       mapping = aes(x = Temp, y = Wing, color = Sex)) +\n  geom_point() +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n\n\nFitting the parallel slopes model\nFitting the parallel slopes model visualized just above is in the same manner as fitting any other multiple regression model (e.g., The NFL win percentage model from Section 3.2): with the lm() function! To fit a regression model that allows the Wing vs. Temp regression line for male and female butteries to differ only by their intercept values, simply include + Sex on the right-hand side of your model formula\n\nwing_by_sex_model &lt;- lm(Wing ~ Temp + Sex, data = ButterfliesBc)\nsummary(wing_by_sex_model)\n\n\nCall:\nlm(formula = Wing ~ Temp + Sex, data = ButterfliesBc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36961 -0.13114 -0.03910  0.08433  0.55390 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.33547    0.13372  144.60  &lt; 2e-16 ***\nTemp        -0.23504    0.05391   -4.36  0.00015 ***\nSexMale     -0.93125    0.08356  -11.14 5.35e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2364 on 29 degrees of freedom\nMultiple R-squared:  0.8316,    Adjusted R-squared:   0.82 \nF-statistic:  71.6 on 2 and 29 DF,  p-value: 6.059e-12\n\n\nThe intercept, slope, and “intercept offset” coefficients in this regression table have identical values to those from the regression table shown beneath the “Assess” heading in Example 3.9 (whew!), but the term labels differ. In the STAT2 textbook, the change in the intercept is labeled IMale, while R does not use any notation to indicate the explanatory variable is an indicator, and labels the term SexMale. R will always use the naming convention {NameOfVariable}{NameOfGroup} when labeling coefficients that reflect a categorical effect.\nThe fitted model equation extracted by the extract_eq() function also follows this notation, naming the explanatory variable \\(\\operatorname{Sex}\\), and indicating this term applies to the “Male” sex group specifically by including \\({}_{Male}\\) in the subscript.\n\nlibrary(equatiomatic)\nextract_eq(wing_by_sex_model, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{Wing}} = 19.34 - 0.24(\\operatorname{Temp}) - 0.93(\\operatorname{Sex}_{\\operatorname{Male}})\n\\]\n\n\nIf you wish to change this notation, you can always use the extract_eq() function in the R console, copy the raw \\(\\LaTeX\\) markup it produces, and edit it as you see fit. For example, the call to extract_eq() above produces the raw \\(\\LaTeX\\) markup:\n$$\n\\operatorname{\\widehat{Wing}} = 19.34 - 0.24(\\operatorname{Temp}) - 0.93(\\operatorname{Sex}_{\\operatorname{Male}})\n$$\nWe could edit the 0.93\\operatorname{Sex}_{\\operatorname{Male}} portion to be 0.93 \\cdot 1_{\\operatorname{Male}}(\\operatorname{Sex}) instead, which produces the equation:\n\\[\n\\operatorname{\\widehat{Wing}} = 19.34 - 0.24(\\operatorname{Temp}) - 0.93 \\cdot 1_{\\operatorname{Male}}(\\operatorname{Sex})\n\\]\nwhich allows the reader to easily identify the final term as an indicator variable\n\n\nAssessing the parallel slopes model\nLuckily, checking the Linearity, Normality, and Equal Variance assumptions by creating Fitted vs. Residuals plots and Normal quantile plots proceeds in familiar fashion, with no deviation from how we’ve previous used the performance package. For example, we can reproduce Figure 3.7 using the check_model() function (though the plots are displayed in a different order):\n\nlibrary(performance)\n\ncheck_model(wing_by_sex_model,\n            check = c(\"qq\", \"normality\", \"linearity\", \"homogeneity\")\n            )\n\n\n\n\n\n\n3.3.1 Removing the Parallel Slopes constraint\nExample 3.10 presents a situation where, unlike the Male and Female butterflies, the simplifying assumption that both groups of observations share the same slope would be misleading. The Kids198 data set holds age (measured in months) and weight measurements of from a sample of 198 children:\n\nlibrary(Stat2Data)\ndata(\"Kids198\")\n\n\n\n\n\n  \n\n\n\nBoth male and female children are measured in this sample (or, boys and girls I suppose), and Figure 3.10 shows that males tend to gain more weight for each month older they get compared to females. In other words: the slope of the Age vs. Weight regression line is steeper for males as compared to females. A first attempt at reproducing Figure 3.10 using ggplot does not go quite as planned:\n\nggplot(data = Kids198,\n       mapping = aes(x = Age, y = Weight, color = Sex)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE, formula = y~x)\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nIn the previous example with male and female butterflies, mapping the Sex variable to the color aesthetic resulted in points that were color-coded by Sex, with one distinct color representing males and another distinct color for females. And, we obtained separate regression lines for male butterflies and female butterflies. But now, we we have males and females represented as two ends of a blue-ish color spectrum and only one regression line. What gives?\nThe issue that in the previous butterflies example, the Sex column held the words \"Male\" and \"Female\"; R treats variables holding character data as inherently categorical. But in the Kids198 data set, the Sex column holds 0’s and 1’s as “abbreviations” for male and female, respectively. R treats these 0’s and 1’s as quantities instead of categories, so we’ll need to make some adjustments to obtain the plot we want.\nThere are several ways to get the job done, but perhaps the most straightforward is to use the recode() and mutate() functions from the dplyr package to change each 0 in the Sex variable to the word \"Male“, and every 1 to the word \"Female\":\n\nKids198 &lt;- Kids198 |&gt;\n  mutate(Sex = factor(Sex, labels = c(\"Male\", \"Female\")))\nselect(Kids198, Weight, Age, Sex)\n\n\n\n  \n\n\n\nWith the Sex variable re-coded to clearly represent categories instead of numbers, the same ggplot() code produces the plot we’re after:\n\nggplot(data = Kids198,\n       mapping = aes(x = Age, y = Weight, color = Sex)\n       ) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE, formula = y~x)\n\n\n\n\nSince we do want to estimate a separate slope of the regression line for males and females, we can return to using the familiar geom_smooth() function to draw the regression lines on top of the observations.\nWe can also fit this model using lm(), but obtaining a model that allows the effect of age to vary across the sexes requires a small change to our model formula notation. Instead of combining the effects of our two explanatory variables, Age and Sex, with a + sign operator, we combine them using the * multiplication operator:\n\nweight_age_model &lt;- lm(Weight ~ Age * Sex, data = Kids198)\n\nThe reason we swapped our + sign for a multiplication operator is revealed by looking at the fitted model equation:\n\nextract_eq(weight_age_model, use_coefs = FALSE)\n\n\\[\n\\operatorname{Weight} = \\alpha + \\beta_{1}(\\operatorname{Age}) + \\beta_{2}(\\operatorname{Sex}_{\\operatorname{Female}}) + \\beta_{3}(\\operatorname{Age} \\times \\operatorname{Sex}_{\\operatorname{Female}}) + \\epsilon\n\\]\n\n\nNotice that in the final term of the equation, the Age variable is multiplied by the Sex variable!3 This multiplicative term is what allows for a different slope between the regression line for male and female children, and this is why R’s model formula adopts the convention to use * when you to estimate a different slope for each group.\nReading the regression table can be a bit tricky for these models, but a little “algorithmic thinking” helps. Take a look at the regression table for the regression table for the Weight ~ Age * Sex below (displaying just the term labels and coefficient estimates):\n\n\n\n\n\n  \n\n\n\n\n\n\n\nIf you look in the (small) figure to the right that visualizes the model, we can see that to fully describe both lines (i.e., both models) we need two intercepts, and two slopes (one for each line!). So, the question becomes: where can we find those pieces of information in the regression table?\nThe (Intercept) term (-1.84) represents the y-axis intercept for the baseline group’s regression line. In this case, the baseline group are the male children. By default, R will put the groups into alphanumeric order, and choose the first group in that list to serve as the baseline group. Knowing this, it’s bit confusing as to why “Female” isn’t the baseline group (since “F” comes before “M”). This is because we used the factor() function to make categories out of the 0’s and 1’s in the original Sex variable, males were coded as 0’s, and this males were “first” and become the baseline group. Even if we didn’t know this, we can also determine “Male” is the baseline group because it’s the only category from the Sex variable not mentioned in any of the term labels\nThe coefficient for the Age variable (.627) represents the slope for the baseline group (male children). How are we supposed to know it’s the baseline group’s slope? In this model, each group gets it’s own slope. In other words, the slope of the regression line depends on the group. So, to know what the slope is, you have to know the group. Since the term doesn’t mention a group, you know it must apply to the baseline group.\nThe final two coefficients are a bit different than the first two: these coefficients represent differences between the intercept and slope of the regression lines for the male and female groups\nThe SexFemale coefficient (-31.9) represents the difference in the intercept between the male and female regression lines. Since this coefficient is labeled with the Female category name, we know the intercept for the female regression line is 31.9 pounds above the intercept for the male regression line. How do we know it is the difference in the intercept, and not the difference in the slope? In this model, the “slope” comes from the gradual change in weight that is produced by a gradual change in age. And, the Age variable is not mentioned any where in this term label; so, we know this term can’t be interpreted as anything slope-related, because it doesn’t involve the Age variable!\nFinally, the coefficient for the Age:SexFemale term (-0.281) represents the difference in the slope between the male and female regression lines. We can infer this because the term label mentions both explanatory variables: the presence of SexFemale in the label tells us we are contrasting the female group with the baseline group (males), and the presence of Age in the label tells us we are measuring the effect of Age on our outcome (weight). The difference in the effect of Age on Weight between the male and female groups is exactly what we mean by “change in slope”!\nWe began our reading of the regression table asking the question: where in the regression table can we find the two intercepts and the two slopes that we need to fully describe his model? We found the intercept and the slope for the baseline group (male children) in the rows labeled (Intercept) and Age. And, there was a bit of a plot twist for the intercept and slope for the female children’s regression line; the regression table doesn’t actually hold the intercept and slope for the female children’s regression line, but instead tells you how the intercept and slope differ from the male children’s regression line.\n\n\n\n\n\n\nWarning\n\n\n\nBased on this example, it might be tempting to conclude “the intercept and slope for the baseline group are in the first two rows, the difference in intercept is in row three, and the difference in slope is in row four”. But, consider what happens when the order of the explanatory variables is reversed in the model formula:\n\nweight_age_model_v2 &lt;- lm(Weight ~ Sex * Age, data = Kids198)\ntidy(weight_age_model_v2)\n\n\n\n  \n\n\n\nThe values in the estimate column are the same, but their order has changed: Now, the slope for the males group is in row three, and the difference in intercept between the groups is in row two. This means that the order of the coefficients depends on the order of the variables in the formula! So, always be sure to use the term labels, your knowledge of the variables types, and your knowledge of the category levels to guide your interpretations. Never rely on the order heuristic!"
  },
  {
    "objectID": "03_multiple_regression.html#new-predictors-from-old",
    "href": "03_multiple_regression.html#new-predictors-from-old",
    "title": "3  Multiple Regression",
    "section": "3.4 New Predictors from Old",
    "text": "3.4 New Predictors from Old\nSection 3.4 extends the idea of allowing your regression to model a change in the relationship between two numeric variables (i.e., a change in slope) across distinct categories of observations into the domain of allowing your regression to model a smooth, gradual changing in the relationship between two numeric variables as yet another numeric explanatory variable also changes. Just as in the case of the interaction between categorical and numeric explanatory variables in section 3.3, the interaction effect between numeric variables is made possible by introducing an explanatory variable that is a product of two explanatory variables in the model.\nExample 3.11 introduces a regression model with two numeric explanatory variables, as well as an interaction term based on those two numeric explanatory variables. This example uses the Perch data set, which holds measurements of the weight (in grams), length (in centimeters), and width (in centimeters) for each of 56 Perch caught at Lake Laengelmavesi in Finland.\n\nlibrary(Stat2Data)\ndata(\"Perch\")\n\n\n\n\n\n  \n\n\n\nWe can fit a multiple regression model that includes an interaction between two numeric variables in the same fashion that we first a the multiple regression model that included an interaction between Sex and Age in Section 3.3: by combining the two variables in our model formula using the * multiplication operator:\n\nperch_weight_model &lt;- lm(Weight ~ Length * Width, data = Perch)\nsummary(perch_weight_model)\n\n\nCall:\nlm(formula = Weight ~ Length * Width, data = Perch)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-140.106  -12.226    1.230    8.489  181.408 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  113.9349    58.7844   1.938    0.058 .  \nLength        -3.4827     3.1521  -1.105    0.274    \nWidth        -94.6309    22.2954  -4.244 9.06e-05 ***\nLength:Width   5.2412     0.4131  12.687  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 44.24 on 52 degrees of freedom\nMultiple R-squared:  0.9847,    Adjusted R-squared:  0.9838 \nF-statistic:  1115 on 3 and 52 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe author’s also show a model based just on the \\(\\operatorname{Length} \\times \\operatorname{Width}\\) product term, without the individual variables that go into the product. Don’t ever do this yourself, but for completeness, here’s how you would execute this bad idea in R:\n\nperch_missing_terms_model &lt;- lm(Weight ~ Length:Width, data = Perch)\ncoef(perch_missing_terms_model)\n\n (Intercept) Length:Width \n -136.926224     3.319287 \n\n\n\n\n3.4.1 Interpreting the coefficients of a two-way numeric interaction model\nThe author’s treat the Weight ~ Length * Width model of the Perch fish weights mostly as a “modeling for prediction” exercise, noting the high \\(R^2\\) statistic this model achieves. But, they don’t offer much advice about how you should think about the meaning of the model’s three coefficients.\n\ntidy(perch_weight_model)\n\n\n\n  \n\n\n\n\nThe (Intercept) term (113.93) is the “z-axis” intercept: it represents the predicted fish weight when the Fish has 0 width and 0 length (a bit of an overestimate if you ask me!)\nThe coefficient for the Length variable (-3.48) represents the change in weight for each additional centimeter of width **when the fish has a length of 0”\nThe coefficient for the Width variable (-94.63) represents the change in weight for each additional centimeter of length **when the fish has a width of 0”\nThe coefficient for the Length:Width variable (i.e., the \\(\\operatorname{Length} \\times \\operatorname{Width}\\) product) can be interpreted in two ways, both equally valid:\n\nThe change in the slope of the Weight vs. Width relationship, each time the Length variable increases by 1 cm.\nThe change in the slope of the Weight vs. Length relationship, each time the Width variable increases by 1 cm.\n\n\nVisualizing the model may help you believe some of these interpretations:\n\nlibrary(regplanely)\n\nregression_plane(perch_weight_model)\n\n\n\n\n\n\n\n3.4.2 Polynomial Regression\nExample 3.13 introduces polynomial regression through the lens of a regression model that attempts to predict atmospheric Carbon Dioxide levels recorded at a monitoring station on Brotjacklriegel mountain in Germany as a function of time; specifically, as a function of the day of the year between April 1st and November 30th, 2001. These data are found in the CO2Germany data set:\n\nlibrary(Stat2Data)\ndata(\"CO2Germany\")\n\n\n\n\n\n  \n\n\n\nCO2 levels and day of the year show a clear curvilinear relationship, with the high levels towards the beginning decreasing as time goes own, gradually tapering off towards the middle of the year, and accelerating back up towards high levels at the end of the year:\n\nggplot(data = CO2Germany, mapping = aes(x = Day, y = CO2)) +\n  geom_point()\n\n\n\n\nThis U-shaped pattern is a good candidate for a polynomial regression model, which will include a \\(Day^2\\) coefficient. What makes a polynomial regression different from a “plain” transformation of the explanatory variable is that the model will include both the linear term \\(Day\\) and the quadratic term \\(Day^2\\).\nAs is often the case when programming, there are a variety of ways to accomplish this in R. One method is simply to add a new column to the data set holding the squared values of the Day variable (just as we did in Chapter 1 when centering the Honda Accord Mileage variable).\n\nCO2Germany &lt;- CO2Germany |&gt;\n  mutate(Day_squared = Day^2)\nCO2Germany\n\n\n\n  \n\n\n\nThen we can use both the Day and Day_squared variables in our model formula. Combining these variables with the + sign operator yields the polynomial model from Example 3.13:\n\nco2_poly_model &lt;- lm(CO2 ~ Day + Day_squared, data= CO2Germany)\ncoef(co2_poly_model)\n\n  (Intercept)           Day   Day_squared \n414.974746588  -0.476034264   0.001157719 \n\n\nWe could also skip the “create a new variable in the data set” step, and square the day variable right in the model formula! But, it takes more adaptation to the model formula syntax than you might expect. Let’s see what happens if you try to use the ^ operator to square the Day variable inside the model formula:\n\nco2_poly_model &lt;- lm(CO2 ~ Day + Day^2, data = CO2Germany)\ncoef(co2_poly_model)\n\n (Intercept)          Day \n368.39001392   0.01622773 \n\n\nOur model has two coefficients, not three? That’s not right - it seems that the whole term involving ^ is completely ignored! Well, as it turns out, the ^ isn’t ignored: it just has a different meaning than “square this number” inside a model formula4. If we want ^ to mean “square this number”, we can wrap the operation inside the I() function (short for “inhibit”):\n\nco2_poly_model &lt;- lm(CO2 ~ Day + I(Day^2), data= CO2Germany)\ncoef(co2_poly_model)\n\n  (Intercept)           Day      I(Day^2) \n414.974746588  -0.476034264   0.001157719 \n\n\nThe term label for this coefficient is a bit awkward this way, but the model is fit correctly! One advantage to doing the squaring directly in the model formula is that some R functions are able to detect the transformation, and use the appropriate methods to reverse or account for the transformation.\n\n\n\n\n\n\nWarning\n\n\n\nThe extract_eq() doesn’t produce the correct equation describing this polynomial model, due to the I() in the term label.\n\nextract_eq(co2_poly_model, use_coefs = TRUE, coef_digits = 5)\n\n\\[\n\\operatorname{\\widehat{CO2}} = 414.97475 - 0.47603(\\operatorname{Day}) + 0.00116(\\operatorname{Day\\texttt{\\^{}}2})\n\\]\nNotice how the 2 isn’t properly superscripted, and the ^ is showing up in the equation? Luckily, it’s not too hard to remove the offending portion of the markup using the string substitution function gsub() before printing it:\n\npoly_eq &lt;- extract_eq(co2_poly_model, use_coefs = TRUE, coef_digits = 5)\ngsub(\"\\\\texttt{^}2}\", \"}^2\", poly_eq, fixed = TRUE)\n\n\\[\n\\operatorname{\\widehat{CO2}} = 414.97475 - 0.47603(\\operatorname{Day}) + 0.00116(\\operatorname{Day}^2)\n\\]\n\n\n\n\n\n3.4.2.1 Visualizing a polynomial regression model\nWe can still visualize our model using the geom_smooth() function; all we need to do is include the quadratic transformation as part of the formula given to the formula argument:\n\nggplot(data = CO2Germany, mapping = aes(x = Day, y = CO2)) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y ~ x + I(x^2), se=FALSE)\n\n\n\n\nNot that when writing a model formula inside the geom_smooth() function, the formula is written with variables referring to plot aesthetics, not to referring to variables in the original data set!\n\n\n\n3.4.3 Complete Second-Order Model\nExample 3.14 introduces a “complete second order” model: one that uses two numeric explanatory variables, a quadratic transformation of both variables, and and interaction between these variables. In Example 3.14, these two numeric variables are the “drop height” and “funnel height” from an experiment seeking to find the combination of these variables that would maximize the amount of time a marble dropped into the funnel would spend circling the funnel before dropping through. These data are found the FunnelDrop data set:\n\nlibrary(Stat2Data)\ndata(\"FunnelDrop\")\n\n\n\n\n\n  \n\n\n\nSince it’s not reasonable to test every combination of drop height and funnel height, modeling their relationship to the circling time is useful to help find a potential maximum that might occur at an untested combination. To obtain this “full second order” model, we’ll have to craft the right hand side our model formula carefully:\n\nfull_second_order_model&lt;- lm(Time ~ Tube * Funnel + I(Funnel^2) + I(Tube^2),\n                             data = FunnelDrop\n                             )\nbroom::tidy(full_second_order_model)\n\n\n\n  \n\n\n\nthe regression_plane() function can be used to visualize this model, provided the quadratic terms have been created inside the model formula using the I(x^2) method:\n\nlibrary(regplanely)\nregression_plane(full_second_order_model)"
  },
  {
    "objectID": "03_multiple_regression.html#correlated-predictors",
    "href": "03_multiple_regression.html#correlated-predictors",
    "title": "3  Multiple Regression",
    "section": "3.5 Correlated Predictors",
    "text": "3.5 Correlated Predictors\nExample 3.15 introduces the HousesNY data set, which contains estimated prices (measured in thousands of dollars) for a sample of 53 city of Canton, NY, along with the size of each house (in 1,000s of square feet), the number of bedrooms, bathrooms, and the size of the lot (in acres).\n\nlibrary(Stat2Data)\ndata(\"HousesNY\")\n\n\n\n\n\n  \n\n\n\nThe predictive accuracy of a regression model estimating the price of a home improves with each attribute of the home added to the model as an explanatory variable. For example, the Price ~ Size + Beds produces a higher \\(R^2\\) value than either the Price ~ Size or Price ~ Beds models (though, adjusted \\(R^2\\) decreases):\n\n\n\n\n  \n\n\n\nNote that if you want to make a compact table of the summaries from multiple models like the one above, you can generalize the code below. It uses the broom::glance() function to create a table of model summaries (which includes the adjusted \\(R^2\\) value), and then uses purrr::map_df() to apply the broom::glance() function to each model in the list and assemble the results into a single table:\n\n\nCode\nprice_beds &lt;- lm(Price ~ Size, data = HousesNY)\nprice_size &lt;- lm(Price ~ Beds, data = HousesNY)\nprice_size_beds &lt;- lm(Price ~ Size + Beds, data = HousesNY)\n\nall_models &lt;- list(price_beds, price_size, price_size_beds)\nmodel_formulas &lt;- purrr::map_chr(all_models, ~as.character(.x$terms))\nnames(all_models) &lt;- model_formulas\n\npurrr::map_df(all_models, broom::glance, .id = \"model\") |&gt;\n  dplyr::select(model, r.squared, adj.r.squared)\n\n\nBut, these models give very different impressions about the importance of the Price vs. Beds relationship\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nAs we learn in Section 3.5, this change in the estimated magnitude of the Beds effect from one model to the next owes to multicollinearity: the explanatory variables in the Price ~ Size + Beds model are linearly related to one another!\nMulticollinearity is not necessarily a bad thing; in general, the ability to estimate the effect of one variable after accounting for the other predictors in the mode is a feature of multiple regression. Using information about how the explanatory variables relate to one another makes this possible. But, there are limits to how reliably a regression model can estimate the effect of one variable after accounting for the other predictors when there are relationships between those explanatory variables. The stronger the relationship between explanatory variables, the more difficult it is to attribute changes in the outcome to one variable or the other\nWhen multicollinearity exists between just two explanatory variables (such as the home size and number of bedrooms variables), constructing a scatter plot matrix of all the variables used in your model may reveal this relationship:\n\nlibrary(GGally)\n\nHousesNY |&gt;\n  select(Price, Beds, Size) |&gt;\n  ggpairs()\n\n\n\n\nBut when your model grows to include more than two explanatory variables, there may not be a strong relationship between any single pair of explanatory variable, but instead a strong relationship between one of your explanatory variables and a combination of multiple other explanatory variables. A scatter plot matrix won’t reveal this relationship to you, so it is better to rely on the Variance Inflation Factor statistic (VIF) as a robust method to detect multicollinearity.\nThe formula for the VIF statistic is fairly simple, but since it needs to be computed for every single explanatory variable in your model, it can be quite tedious to implement when models grow large. Instead, we’ll use the vif() function from the car package (Fox and Weisberg 2019).\nInstead of of demonstrating how to compute the VIF using the CountyHealth data set (as done in Example 3.17), we’ll build another model using the HousesNY data set, but this time using all the remaining variables in the data as explanatory variable of the home price. Notice the handy shortcut . on the right hand side of the model formula, which R expands to mean “all other variables in the data set”:\n\nprice_all_variables &lt;- lm(Price ~ ., data = HousesNY)\n\nlibrary(car)\nvif(price_all_variables)\n\n    Beds    Baths     Size      Lot \n2.283420 1.223342 2.425255 1.058951 \n\n\nThese VIF statistics are relatively low: a VIF of 5 is a cutoff for a “concerning” amount of multicollinearity between predictors, and 10 is cutoff for “very concerning” amount of multicollinearity between predictors. So, a VIF of at most 2.4 means we should consider our coefficient estimates in the model reliable."
  },
  {
    "objectID": "03_multiple_regression.html#testing-subsets-of-predictors",
    "href": "03_multiple_regression.html#testing-subsets-of-predictors",
    "title": "3  Multiple Regression",
    "section": "3.6 Testing Subsets of Predictors",
    "text": "3.6 Testing Subsets of Predictors\nSection 3.6 introduces the Nested F-test as formal model comparison technique. The Nested F Statistic measures the amount of additional variability explained by adding one or more predictor to an existing more, and the F-test based on this statistic helps you decide if this additional variability explained is large enough for you to prefer this more complex model.\nWhile the formula given for the Nested F statistic given in Section 3.6 is a bit complicated, there is good news: R makes it easy to perform a nested F-test without implementing the formula yourself! We’ve previously used the anova() function to perform F-tests on the individual sources of variable identified in a single model (i.e., F-test on the explanatory variables). But, when you supply several models to the anova function, it performs a Nested F-test comparing those models.\nConsider this example based on the data from the HousesNY data set, that uses a nested F-test to compare two regression models:\n\nprice_size &lt;- lm(Price ~ Size, data = HousesNY)\nprice_size_beds_lot &lt;- lm(Price ~ Size + Beds + Lot, data = HousesNY)\n\nanova(price_size, price_size_beds_lot)\n\n\n\n  \n\n\n\nThe statistic given in the F column is the nested F-statistic, and the p-value helps you decide whether you should reject the null hypothesis that the \\(\\beta_{Beds} = \\beta_{Lots} = 0\\) in favor of the alternative that either \\(\\beta_{Beds} \\ne 0\\) or \\(\\beta_{lots} \\ne 0\\). Here, the large p-value gives us no reason to reject the null hypothesis, indicating we should favor the simpler Price ~ Size model.\nYou can provide the models in any order (the F-statistic and p-values won’t change), but ordering the models in ascending order of complexity (models with fewer predictors first, models with more predictors last) is preferred. That way, you won’t have negative sums of square and degrees of freedom, which can be a bit confusing:\n\nanova(price_size, price_size_beds_lot)\nanova(price_size_beds_lot, price_size)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nIf you provide 3 or more models, R will perform nested F-tests in pairs:\n\nprice_size &lt;- lm(Price ~ Size, data = HousesNY)\nprice_size_beds_lot &lt;- lm(Price ~ Size + Beds + Lot, data = HousesNY)\nprice_all_variables &lt;- lm(Price ~ ., data = HousesNY)\n\n\nanova(price_size, price_size_beds_lot, price_all_variables)\n\n\n\n  \n\n\n\nSo the F-test in row 2 of the table compares model 2 (Price ~ Size + Beds + Lot) to model 1 (Price ~ Size), while the F-test in row 3 of the table compares model 3 (Price ~ Beds + Baths + Size + Lot) model 2 (Price ~ Size + Beds + Lot).\nIf you ever attempt to supply non-nested model to the anova() function, you won’t get an error, but you also won’t get any F-statistics - R does its best to protect you from yourself!\n\nprice_size &lt;- lm(Price ~ Size, data = HousesNY)\nprice_beds_lot &lt;- lm(Price ~ Beds + Lot, data = HousesNY)\n\nanova(price_size, price_beds_lot)\n\n\n\n  \n\n\n\n\n\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nKim, Albert Y., Chester Ismay, and Max Kuhn. 2021. “Take a Moderndive into Introductory Linear Regression with r.” The Journal of Open Source Education 4 (41, 115). https://doi.org/10.21105/jose.00115.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ’Ggplot2’. https://CRAN.R-project.org/package=GGally."
  },
  {
    "objectID": "03_multiple_regression.html#footnotes",
    "href": "03_multiple_regression.html#footnotes",
    "title": "3  Multiple Regression",
    "section": "",
    "text": "Tidyverse aficionados will prefer the expand_grid() function from the tidyr package, which performs the same task, but never converts strings to factors, does not add any additional attributes, can expand any generalized vector, including data frames and lists.↩︎\nAugust 23, 2022↩︎\nRemember of course, Age isn’t multiplied by “Male” or “Female”, age is multiplied by 1 or 0, because the Sex variable is wrapped in an indicator function when used in the model↩︎\nThe quote the help page for ?formula: “The ^ operator indicates crossing to the specified degree. For example (a+b+c)^2 is identical to (a+b+c)*(a+b+c) which in turn expands to a formula containing the main effects for a, b and c together with their second-order interactions.”↩︎"
  },
  {
    "objectID": "04_additional_topics_in_regression.html#added-variable-plots",
    "href": "04_additional_topics_in_regression.html#added-variable-plots",
    "title": "4  Additional Topics in Regression",
    "section": "4.1 Added Variable Plots",
    "text": "4.1 Added Variable Plots"
  },
  {
    "objectID": "04_additional_topics_in_regression.html#techniques-for-choosing-predictors",
    "href": "04_additional_topics_in_regression.html#techniques-for-choosing-predictors",
    "title": "4  Additional Topics in Regression",
    "section": "4.2 Techniques for Choosing Predictors",
    "text": "4.2 Techniques for Choosing Predictors"
  },
  {
    "objectID": "04_additional_topics_in_regression.html#cross-validation",
    "href": "04_additional_topics_in_regression.html#cross-validation",
    "title": "4  Additional Topics in Regression",
    "section": "4.3 Cross-validation",
    "text": "4.3 Cross-validation"
  },
  {
    "objectID": "04_additional_topics_in_regression.html#identifying-unusual-points-in-regression",
    "href": "04_additional_topics_in_regression.html#identifying-unusual-points-in-regression",
    "title": "4  Additional Topics in Regression",
    "section": "4.4 Identifying Unusual Points in Regression",
    "text": "4.4 Identifying Unusual Points in Regression"
  },
  {
    "objectID": "04_additional_topics_in_regression.html#coding-categorical-predictors",
    "href": "04_additional_topics_in_regression.html#coding-categorical-predictors",
    "title": "4  Additional Topics in Regression",
    "section": "4.5 Coding Categorical Predictors",
    "text": "4.5 Coding Categorical Predictors"
  },
  {
    "objectID": "04_additional_topics_in_regression.html#randomization-test-for-predictors",
    "href": "04_additional_topics_in_regression.html#randomization-test-for-predictors",
    "title": "4  Additional Topics in Regression",
    "section": "4.6 Randomization Test for Predictors",
    "text": "4.6 Randomization Test for Predictors\nThe treatment of randomization testing is a bit sparse; for a introduction to hypothesis testing with randomization that has more exposition, consult an Introduction to Modern Statistics or ModernDive.\nThe infer package (Couch et al. 2021) provides a simple and clear way to perform a randomization test for the slope of a regression model. The idea is that the process of testing the hypothesis “Is there a linear relationship between two numeric variables?” in the NHST framework (without making any distributional assumptions) can be broken down into 5 steps:\n\nSpecifying the variables of interest\nHypothesizing about their relationship\nGenerating samples of data from this hypothesized state of the world\nCalculating a summary of each sample that measures their relationship in that sample\nCalculating the probability of observing your original data1, given the data you’ve observed from your hypothesized state of the world via your simulations in step 3.\n\nSteps 1 through 4 describe the process behind creating the null-hypothesis distribution (i.e., the distribution of statistics you would expect to observe if the null hypothesis were true), while step 5 describes computing the p-value, so that you can reach a “reject” or “fail to reject” decision for your test. Each of these steps along the way has their own function in the infer package; to perform a hypothesis test, you simply chain these steps together!\nSometimes to understand how something, it’s best to see the whole, big picture, and slowly break it down piece by piece from there. I think this “forest, then tree” approach is useful for understanding the infer package, so to see how the infer package executes a randomization test, we’ll jump ahead to a completed example, and break down each part afterwards.\n\n4.6.1 A randomization test for the slope\nLet’s base our example on the SAT and GPA data used in Example 4.11\n\nlibrary(Stat2Data)\ndata(\"SATGPA\")\n\n\n\n\n\n  \n\n\n\nLet’s begin with code that executes Steps 1-4 on our hypothesis testing roadmap:\n\nlibrary(infer)\n\nnull_dist &lt;- SATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"slope\")\n\nand break this code down, one line at a time.\n\nStep 1: Specify the relationship of interest\n\nnull_dist &lt;- SATGPA  |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n\nThe specify() function is where you set up the structure of the model using a the familiar formula notation. We’re specifying that we’re interested in the GPA and VerbalSAT relationship, and that we think the VerbalSAT score explains the GPA outcome.\n\n\nStep 2: State your null hypothesis\n\n  hypothesize(null = \"independence\") |&gt;\n\nEvery null hypothesis test begins with an assumption that the null hypothesis is true. Most of the time, this assumption is implicit, but randomization tests with infer require you to make this assumption explicit.\nWriting null = \"independene\" in this context means you are saying “knowing a persons Verbal SAT score tells me nothing about what their GPA is likely to be”. In other words, you’re saying the the slope of the regression line relating Verbal SAT to GPA is 0 in the entire population of SAT test takers.\n\n\nStep 3: Generate new samples of data by permuting our data\n\n  generate(reps = 1000, type = \"permute\") |&gt;\n\nThis is the most import step in our hypothesis testing “pipeline”: This is where we take each Verbal SAT score from our data, and pair it with a random GPA score that is sampled (without replacement) from our original data as well. We repeated this process 1,000 times, which generates 1,000 new samples of data that are permutations of our original data.\nThis permutation process is helpful, because it generates 1,000 new data sets where each Verbal SAT score is completely unrelated to it’s paired GPA value. This how the Null Hypothesis says data is generated in the “real world”, and it’s exactly how we’ve generated our permutations. So, these 1,000 new data sets will show us exactly what kinds of GPA and Verbal SAT pairs we’d expect, if the null hypothesis is true!\n\n\nStep 4: Calculate the GPA ~ VerbalSAT slope from each permutation\n\n  calculate(stat = \"slope\")\n\nSince our hypothesis test is about the relationship between GPA and Verbal SAT score, the slope is the statistic that best represents that idea. So, we summarize all of our 1,000 permuted data sets by fitting the GPA ~ VerbalSAT regression model to each one, and extracting the slope coefficient from each. So, the final result from our “pipeline” is 1,000 slopes observed from a world where the null hypothesis of “no relationship” is true!\n\nvisualize(null_dist)\n\n\n\n\n\n\nStep 5: Computing the p-value\nComputing the p-value for our hypothesis test means we need to compare the observed slope from our real, unpermuted data to the distribution of slopes we obtained by summarizing our permuted data. Thus, we need the value of the observed slope! You could always do this the “traditional” way, using the lm() function, but since we only need the slope (and not the standard error, or \\(R^2\\), etc.), let’s demonstrate how you would do this using just specify() and calculate() from the infer package:\n\nobserved_slope &lt;- SATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  calculate(stat = \"slope\")\n\nobserved_slope\n\n\n\n  \n\n\n\nFinally, we compute the p-value of our slope by using the get_p_value() function. This function counts number of times our a slope more extreme than the “real” slope resulted from the random permutation process, and dividing that by the total number of permutations performed. Since our question was a generic “is there a relationship?” question (not “is there a positive/negative” relationship), we want to count values in both tails of the null hypothesis distribution as “extreme”.\n\nget_p_value(x = null_dist, obs_stat = observed_slope, direction = \"both\")\n\n\n\n  \n\n\n\nThis p-values corresponds to the area in the histogram of permuted slopes that is more extreme than 9^{-4}.\n\nvisualise(null_dist) +\n  shade_p_value(obs_stat = observed_slope, direction = 'both')\n\n\n\n\nWith such a large p-value, we fail to reject the null hypothesis, and conclude there is no evidence the two quantities are related in the population of SAT test takers.\n\n\n\n4.6.2 A randomization test for the correlation\nExample 4.11 actually demonstrates a randomization test for the Verbal SAT vs. GPA correlation, the GPA ~ VerbalSAT slope. But, it is easy to execute such a test simply by changing the stat argument of the calculate function from \"slope\" to \"correlation\"!\n\nnull_correlation_dist &lt;- SATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"correlation\")\n\nobserved_correlation &lt;- SATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  calculate(stat = \"correlation\")\n\nget_p_value(null_correlation_dist, obs_stat = observed_correlation,\n            direction = \"both\"\n            )\n\n\n\n  \n\n\nvisualise(null_correlation_dist) +\n  shade_p_value(obs_stat = observed_correlation, direction = \"both\")\n\n\n\n\n\n\n4.6.3 Randomization for inference in Multiple Regression\nThis random permutation-based procedure can be extended to inference on the coefficients of a multiple regression model as well. All you need to do is:\n\nAdapt your formula argument in the specify() function to reflect the structure of your model\nComplete your pipeline with the fit() function, instead of the calcuate() function (which makes sense, since there isn’t one single summary statistic for all the relationships encoded by a multiple regression model)\n\nFor example, we could perform a permutation test on all four of the coefficients in the GPA ~ VerbalSAT * MathSAT model:\n\nnull_models &lt;- SATGPA |&gt;\n  specify(GPA ~ VerbalSAT * MathSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\nobserved_model &lt;- SATGPA |&gt;\n  specify(GPA ~ VerbalSAT * MathSAT) |&gt;\n  fit()\n\nget_p_value(null_models, obs_stat = observed_model,\n            direction = \"both\"\n            )\n\n\n\n  \n\n\nvisualise(null_models) +\n  shade_p_value(obs_stat = observed_model, direction = \"both\")\n\n\n\n\n\n\n4.6.4 A note about controlling randomness\nWe’re used to our R code producing identical results every time we run it. For example, we don’t expect the correlation to change the first and second times we calculate it:\n\nSATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  calculate(stat = \"correlation\")\n\n\n\n  \n\n\n\n\nSATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  calculate(stat = \"correlation\")\n\n\n\n  \n\n\n\nBut, things get a bit trickier when we’re constructing the null hypothesis distribution. Consider the example below, where we construct 3 permutations and summarize each one with the correlation coefficient, and then run the exact same code again:\n\nSATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 3, type = \"permute\") |&gt;\n  calculate(stat = \"correlation\")\n\n\n\n  \n\n\n\n\nSATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 3, type = \"permute\") |&gt;\n  calculate(stat = \"correlation\")\n\n\n\n  \n\n\n\nHere, we get 3 different correlations the second time! What’s going on?\nThe difference is: we’re relying on R’s random number generator to help us randomly shuffle the data to construct our three permutations. The second time we run the code, R generates different random numbers (as we would expect!), yielding different permutations that we obtained during the first execution, and thus three different correlations!\nHaving no idea what result you’ll get each time you run your code makes it difficult to write about your results, or re-produce them for people who are interested in the “proof” of your work. Luckily, we can take some control over R’s random number generator using the set.seed() function.\nThe set.seed() function “seeds” R’s random number generator with a fixed value, making the sequence of numbers it generates reproducible between different runs of the same code. Watch what happens when we “seed” the generate with an 11:\n\nset.seed(11)\nSATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 3, type = \"permute\") |&gt;\n  calculate(stat = \"correlation\")\n\n\n\n  \n\n\nset.seed(11)\nSATGPA |&gt;\n  specify(GPA ~ VerbalSAT) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 3, type = \"permute\") |&gt;\n  calculate(stat = \"correlation\")\n\n\n\n  \n\n\n\nNow, we get the same correlations each time! It’s recommended that whenever you’re working with R’s random number generator, you set a seed to make your work reproducible. So, don’t forget about the set.seed() function as you’re working on your randomization tests!\nNote that you can “seed” R’s random number generator with any positive integer."
  },
  {
    "objectID": "04_additional_topics_in_regression.html#bootstrap-for-regression",
    "href": "04_additional_topics_in_regression.html#bootstrap-for-regression",
    "title": "4  Additional Topics in Regression",
    "section": "4.7 Bootstrap for Regression",
    "text": "4.7 Bootstrap for Regression\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661."
  },
  {
    "objectID": "04_additional_topics_in_regression.html#footnotes",
    "href": "04_additional_topics_in_regression.html#footnotes",
    "title": "4  Additional Topics in Regression",
    "section": "",
    "text": "Not your original data exactly; rather, a statistic based on your original data↩︎"
  },
  {
    "objectID": "05_one-way_ANOVA.html#overview-of-anova",
    "href": "05_one-way_ANOVA.html#overview-of-anova",
    "title": "5  One-way ANOVA and Randomized Experiments",
    "section": "5.1 Overview of ANOVA",
    "text": "5.1 Overview of ANOVA\nNo analyses are carried out in this section, so this section is omitted from the R Companion. However, this stub is included to keep the section numbers in each chapter consistent with the STAT2 text."
  },
  {
    "objectID": "05_one-way_ANOVA.html#the-one-way-randomized-experiment-and-its-observational-sibling",
    "href": "05_one-way_ANOVA.html#the-one-way-randomized-experiment-and-its-observational-sibling",
    "title": "5  One-way ANOVA and Randomized Experiments",
    "section": "5.2 The One-way Randomized Experiment and Its Observational Sibling",
    "text": "5.2 The One-way Randomized Experiment and Its Observational Sibling\nNo analyses are carried out in this section, so this section is omitted from the R Companion. However, this stub is included to keep the section numbers in each chapter consistent with the STAT2 text."
  },
  {
    "objectID": "05_one-way_ANOVA.html#fitting-the-model",
    "href": "05_one-way_ANOVA.html#fitting-the-model",
    "title": "5  One-way ANOVA and Randomized Experiments",
    "section": "5.3 Fitting the Model",
    "text": "5.3 Fitting the Model\nThere are many different ways to carry out an ANOVA in R, several of which involve R packages outside of the base set of packages installed with R by default. Here, we’ll focus on how to carry out the so-called “one-way” or “one-factor” ANOVA that is the topic of Chapter 5.3 using only the tools included with base R.\nThe first analysis we’ll replicate is the ANOVA based on the Undoing data set, which is presented incrementally across Examples 5.6 and 5.9. The Undoing data set holds results from an observational study of one patient’s progress using psychotherapy to treat OCD. Transcript’s from the patient’s psychotherapy sessions were rated by other therapists one a 1 to 4 scale measuring the inverse severity of the patient’s OCD symptoms (a rating of 1 means “severe symptoms”, and a rating of 4 means “no symptoms”). The transcripts were dividing into six chronologically ordered groups, allowing the researchers to measure how the symptom severity ratings given by the therapists changed as they read sessions progressively further along in the patient’s treatment.\nFor simplicity, the ANOVA presented in Section 5.3 only uses the rating data from the first, third, and fifth group of session ratings. The data from these three time points is shown below in Figure 5.1.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"Undoing\")\nUndoing_subset &lt;- Undoing |&gt;\n  filter(Group %in% c(\"I\", \"III\", \"IV\"))\n\nggplot(data = Undoing_subset,\n       mapping = aes(x = Group, y = Score)\n       ) +\n  geom_jitter(width = .1)\n\n\n\n\nFigure 5.1: A subset of the observations in the Undoing data set reflecting OCD symptom severity ratings from three different time points.\n\n\n\n\nOne such to execute an ANOVA with these data is to\n\nFirst use the lm() function to a linear model that regresses your outcome value on your categorical explanatory variable\nThen carry out the ANOVA and F-test for each explanatory variable in your model by calling the anova() function on your linear model object.\n\nFor example, we can replicate the ANOVA table from Example 5.9 by doing:\n\nundoing_model &lt;- lm(Score ~ Group, data = Undoing_subset)\nanova_table &lt;- anova(undoing_model)\nanova_table\n\n\n\n  \n\n\n\nOne difference between this ANOVA table, and the one presented in Example 5.9, is that this table lacks a “Total” row showing the overall degrees of freedom and sums of squares. This is not a serious loss, and if that information is ever need, you can sum the degrees of freedom and sum of squares columns in the ANOVA table to recover it:\n\nanova_table |&gt;\n  summarise(total_df = sum(Df),\n            total_ss = sum(`Sum Sq`)\n            )\n\n\n\n  \n\n\n\nR also includes an aov() function which combines step 1 (fitting the regression model) and 2 (calculating the sums of squares, mean squares, and F-statistics to construct the ANOVA table) into one step. We’ll demonstrate the aov() function by replicating the ANOVA for the Leafhoppers data set that is presented across Examples 5.5 and 5.5.\nThe Leafhoppers data set holds results from an experiment that studied how the lifespan of Potato Leafhopper insects was impacted by earning diets containing solely sugar compounds (Sucrose, Glucose, and Fructose). The data set 8 observations (two observations for each sugar compound, plus a control condition), and are shown below in Figure 5.2.\n\ndata(\"Leafhoppers\")\n\nggplot(data = Leafhoppers,\n       mapping = aes(x = Diet, y = Days)\n       ) +\n  geom_point() +\n  scale_x_discrete(limits = c(\"Control\", \"Sucrose\", \"Glucose\", \"Fructose\"))\n\n\n\n\nFigure 5.2: Leafhopper survival time (in days) plotted against the leafhopper’s diet\n\n\n\n\nFinally, we’ll replicate the ANOVA table from Example 5.7 using the aov() function. The aov() function effectively serves as a drop-in replacement for the lm() function: you supply the aov() function with a model formula describing your outcome and explanatory variables (as well as the data set where they cane be located), and it supplies you with an ANOVA table:\n\nleafhoppers_anova &lt;- aov(Days ~ Diet, data = Leafhoppers)\nsummary(leafhoppers_anova)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3   3.92   1.307   17.42 0.00925 **\nResiduals    4   0.30   0.075                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "05_one-way_ANOVA.html#assessing-and-using-the-model",
    "href": "05_one-way_ANOVA.html#assessing-and-using-the-model",
    "title": "5  One-way ANOVA and Randomized Experiments",
    "section": "5.4 Assessing and Using the Model",
    "text": "5.4 Assessing and Using the Model\nSection 5.4 explains that the F-statistic created from the \\(\\frac{MS_{Model}}{MS_{Error}}\\) ratio can be used to test the null hypothesis that \\(\\mu_1 = \\mu_2 = ... \\mu_j\\) where \\(j\\) indexes the groups of data defined by the categorical explanatory variable in the model.\nThe p-values shown in R’s ANOVA tables answer the question “What is the probability of observing an F statistic as extreme or more extreme than the one computed from this sample of data, if \\(\\mu_1 = \\mu_2 = ... \\mu_j\\)?”. These p-values are computed by finding \\(P(F &gt;= \\frac{MS_{Model}}{MS_{Error}})\\) using an F distribution with \\(j-1\\) and \\(n-j\\) degrees of freedom. However, for this probability to accurately measure \\(P(F &gt;= \\frac{MS_{Model}}{MS_{Error}})\\), then \\(\\frac{MS_{Model}}{MS_{Error}})\\) must actually follow the pattern of an \\(F_{j-1, n-j}\\) distribution. Whether or not this is true boils down to the pattern of residual errors within each group of data, the \\(y_{ij} - \\mu_j\\) values. Specifically,\n\nThe residuals (both within and across groups) must reflect a random process\nThe residuals (both within and across groups) must be independent\nThe residuals have the same degree of variability in each group\nThe residuals are normally distributed\n\nIf these conditions look familiar to you, it’s because they are; these are fundamentally the same assumptions (independence, equal variance, normality) that supported the use of t-tests on the \\(b_k\\) coefficients of a linear regression model (which makes sense, given that an ANOVA is just a particular way of analyzing the fit of a linear model). The only difference here is that the equality of variance we’re interested in regards the error variability observed within each group of residuals, rather than assessing the equality of the residual error between each observation and a single, global regression line.\nThere isn’t any universal method of assessing the validity of the first two conditions for inference; in other words, there aren’t any “stock” visualizations or hypothesis tests that can be applied to any ANOVA to assess whether randomness and independence are reasonable assumptions to make about your residual errors. Rather, you should ask yourself these questions:\n\nWhen observations differ from the group mean, to what do I attribute this difference?\n\nIf you can explain exactly why a particular observation differs from the group mean, then this residual probably isn’t random\n\nIs there anything that directly links two residuals errors together, aside from the fact that they are in the same group?\n\nFor example, ff two residual errors come from the same person, or the same household, then these residual errors will probably be more similar to each other than any other randomly chosen residual errors, and thus, are not independent.\n\n\nAs recommended by the STAT2 text, the Normality and equal variance assumptions can be assessed by plotting distribution of residuals within each group, and well as a Normal-quantile plot for the residuals respectively. The quickest way to create these two plots are with the check_homogeneity() and check_normality() functions from the performance package. To demonstrate, we’ll replicate the ANOVA for the fruit flies data set used in Example 5.11, and reproduce Figure 5.14. First, we’ll need to load the data, and fit the ANOVA model:\n\ndata(\"FruitFlies\")\nfruitfly_anova &lt;- aov(Longevity ~ Treatment, data = FruitFlies)\nsummary(fruitfly_anova)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment     4  11939  2984.8   13.61 3.52e-09 ***\nResiduals   120  26314   219.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNext, we’ll pass our aov model object to the check_homogeneity() and check_normality() functions. These functions themselves do not create the visualizations; rather, the plot() function must be called on the objects returned by check_homogeneity() and check_normality() to see the visualizations. We’ll use the grid.arrange() function (from the gridExtra package) to place them side-by-side in the same figure:\n\nlibrary(performance)\nlibrary(see)\nlibrary(gridExtra)\n\nvariance_check &lt;- check_homogeneity(fruitfly_anova)\nnormality_check &lt;- check_normality(fruitfly_anova)\n\ngrid.arrange(plot(variance_check),\n             plot(normality_check, type = \"qq\"),\n             ncol = 2\n             )\n\n\n\n\nThe check_homogeneity() function describes the residuals in each group using violin plots instead of box plots. If you aren’t familiar with reading violin plots, a good introduction is found here (just ignore the Python code if you don’t speak Python).\nIf you’d rather use box plots instead of violin plots, we’ll have to construct them “manually”. We can do this without too much trouble by using the broom::augment() function to obtain a data frame holding residual errors for each observation in the data, and then using ggplot() to plot the distribution of these residuals for each group of fruit flies.\n\nlibrary(broom)\n\nresiduals &lt;- augment(fruitfly_anova)\n\nWarning: The `augment()` method for objects of class `aov` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\nhead(residuals, 5)\n\n\n\n  \n\n\nboxplots &lt;- ggplot(data = residuals,\n                   mapping = aes(x = Treatment, y = .resid)\n                   ) +\n  geom_boxplot()\n\ngrid.arrange(boxplots,\n             plot(normality_check, type = \"qq\"),\n             ncol = 2\n             )\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\nBoth plotting approaches (the violin plot and box plot methods) encourage use that the conditions for inference (equal variance and normality of the residuals) are quite reasonably met in this case.\nWhen running the code above, the augment() function may give you warning, cautioning that you that “The augment() method for objects of class aov is not maintained by the broom team…”. In this situation, we can safely ignore this warning. But, that may not be the case for other, more complex ANOVA analyses carried out in Chapter 8, specifically those that divide the residual errors into different “strata”."
  },
  {
    "objectID": "05_one-way_ANOVA.html#using-plots-to-help-choose-a-scale-for-the-response",
    "href": "05_one-way_ANOVA.html#using-plots-to-help-choose-a-scale-for-the-response",
    "title": "5  One-way ANOVA and Randomized Experiments",
    "section": "5.5 Using Plots to Help Choose a Scale for the Response",
    "text": "5.5 Using Plots to Help Choose a Scale for the Response"
  },
  {
    "objectID": "05_one-way_ANOVA.html#multiple-comparisons-and-fishers-least-significant-difference",
    "href": "05_one-way_ANOVA.html#multiple-comparisons-and-fishers-least-significant-difference",
    "title": "5  One-way ANOVA and Randomized Experiments",
    "section": "5.6 Multiple Comparisons and Fisher’s Least Significant Difference",
    "text": "5.6 Multiple Comparisons and Fisher’s Least Significant Difference\nA “significant” F-test from an ANOVA variance indicates “At least one group of observations has a mean value that is ‘significantly’ different than the mean value of least one other group of observations”. In other words, a “significant” F-test indicates “a difference exists”, but doesn’t identify the nature of this difference.\nFor example, the “significant” F-test Leafhoppers ANOVA in Section 5.3 indicates “There is a difference among the average lifespans of Leafhoppers who consume different sugar compounds”, but we don’t yet know if this difference exists amongst the Glucose and Sucrose groups, the Fructose and Sucrose groups, or both! To address this question, we’ll need to contrast the means in each group.\nSection 5.6 focuses on a method of contrasting each pair of group means (called Fisher’s Least Significant Difference) that allows you to hold the false positive rate across the family of contrasts of contrasts a chosen small value (e.g., .05). We’ll execute the Least Significant Difference contrasts “by hand” for the Leafhoppers ANOVA.\nFisher’s LSD procedure compares each difference in means to a “threshold” value called the Least Significant Difference. This Least Significant Difference (LSD) is defined as \\[\nLSD = min_{j, k} \\bigg\\{t^* \\cdot \\sqrt{\\frac{MS_{Error}}{n_j} + \\frac{MS_{Error}}{n_k}}\\bigg\\}\n\\]\nwhere\n\n\\(N\\) is total number of observations in the data set.\n\\(J\\) is the total number of groups (i.e., the number of levels belonging to the explanatory variable).\n\\(t^*\\) is the \\(1-\\frac{\\alpha}{2}\\) quantile of the t distribution with \\(N-J\\) degrees of freedom, and \\(\\alpha\\) is the significance level used for the F-test from the ANOVA.\n\\(MS_{Error}\\) is the Mean Square Error (computed from the ANOVA model).\n\\(n_j\\) and \\(n_k\\) are the number of observations in the \\(i\\)-th and \\(j\\)-th group respectively.\n\nFor an unbalanced design (where each group has a different number of observations), we would have to compute \\(\\frac{j(j-1)}{2}\\) different candidate LSD values times (one for each possible pair of groups) and take the minimum of those \\(\\frac{j(j-1)}{2}\\) values as our LSD. However, since the Leafhopper data set is balanced (each group has 2 observations), the LSD won’t change, so we can compute it exactly once.\nThus, for the Leafhoppers ANOVA, we have\n\nN &lt;- 8\nJ &lt;- 4\nn_j &lt;- 2\n\nt_star &lt;- qt(1 - (.05/2), df = N-J)\nt_star\n\n[1] 2.776445\n\nMS_error &lt;- summary(leafhoppers_anova)[[1]][2 , \"Mean Sq\"]\nMS_error\n\n[1] 0.075\n\nLSD &lt;- t_star * sqrt(MS_error/2 + MS_error/2)\nLSD\n\n[1] 0.7603608\n\n\nNow that we have the LSD threshold, we compare it against each of the 6 differences in means. The quickest way to compute each the pairwise differences in means is with the emmeans() package:\n\nlibrary(emmeans)\n\nleafhoppers_means &lt;- emmeans(leafhoppers_anova, specs = ~ Diet)\nleafhoppers_means\n\n Diet     emmean    SE df lower.CL upper.CL\n Control     2.0 0.194  4     1.46     2.54\n Fructose    2.2 0.194  4     1.66     2.74\n Glucose     2.8 0.194  4     2.26     3.34\n Sucrose     3.8 0.194  4     3.26     4.34\n\nConfidence level used: 0.95 \n\npaiwise_differences &lt;- contrast(leafhoppers_means,\n                                method = \"pairwise\",\n                                infer = FALSE\n                                )\npaiwise_differences\n\n contrast           estimate    SE df\n Control - Fructose     -0.2 0.274  4\n Control - Glucose      -0.8 0.274  4\n Control - Sucrose      -1.8 0.274  4\n Fructose - Glucose     -0.6 0.274  4\n Fructose - Sucrose     -1.6 0.274  4\n Glucose - Sucrose      -1.0 0.274  4\n\n\nFinally, we can compare each difference in means (which are found in the estimate column) against our LSD threshold:\n\npaiwise_differences |&gt;\n  as.data.frame() |&gt;\n  mutate(LSD_significant = abs(estimate) &gt; LSD)\n\n\n\n  \n\n\n\nThus, according to Fisher’s LSD procedure, we can infer than Leafhoppers eating Glucose and Sucrose (but not those eating Fructose) will live longer on average than Leafhoppers eating the control diet, and Leafhoppers eating Glucose and Sucrose will live longer on average than Leafhoppers eating Fructose. In other words, the significant F-test from our ANOVA appears to have been driven by longer lifespans for the insects consuming Glucose and Sucrose compared to the other two groups (control and Fructose).\n\n5.6.1 Other Methods of Protecting the Family-wise Error Rate\nFisher’s LSD procedure protects your family-wise false positive rate, but only under narrow circumstances: your F-test was significant, and you are contrasting all possible pairs of means. As the STAT2 authors mention in a footnote at the end of section 5.7, there are other methods for protecting the family-wise error rate from becoming inflated by multiple comparison which are more widely applicable. See Chapter 8.2 for examples of how to perform similar contrast analyses such as the Leafhopper contrasts performed above, but using other more robust methods to deal with the problems created with multiple comparisons."
  },
  {
    "objectID": "06_two-way_ANOVA.html",
    "href": "06_two-way_ANOVA.html",
    "title": "6  Blocking and Two-way ANOVA",
    "section": "",
    "text": "This section is not yet implemented."
  },
  {
    "objectID": "07_ANOVA_with_interaction.html",
    "href": "07_ANOVA_with_interaction.html",
    "title": "7  ANOVA with Interaction and Factorial Designs",
    "section": "",
    "text": "This section is not yet implemented."
  },
  {
    "objectID": "08_additional_topics_in_ANOVA.html",
    "href": "08_additional_topics_in_ANOVA.html",
    "title": "8  Additional Topics in Analysis of Variance",
    "section": "",
    "text": "This section is not yet implemented."
  },
  {
    "objectID": "09_logistic_regression.html#choosing-a-logistic-regression-model",
    "href": "09_logistic_regression.html#choosing-a-logistic-regression-model",
    "title": "9  Logistic Regression",
    "section": "9.1 Choosing a Logistic Regression Model",
    "text": "9.1 Choosing a Logistic Regression Model\nChapter 9 introduces the topic of logistic regression though the lens of the question: “What is the relationship between a teenager’s age in years, and the chances of getting 7 or more hours of sleep a night”. In this situation, the outcome variable we seek to model and explain is a binary categorical variable; in other words, a variable that measures a categorical attribute that can only take on two possible values. “Getting 7 or more hours of sleep a night” is a binary categorical variable here because respondents were only allowed to give “yes” or “no” responses to the question.\nThe data collected from this survey are first summarized in a joint frequency table, showing the number of respondents who answered “Yes” and “No”, broken down into groups based on the respondents age. We could re-create the same table by first counting the number of rows with each possible combination of age (14,15,16,17, or 18) and hours slept (“Yes” = seven or more hours, “No” = less than seven hours).\n\nlibrary(Stat2Data)\ndata(\"LosingSleep\")\n\n\nlibrary(dplyr)\n\nsleep_counts &lt;- LosingSleep |&gt;\n  count(Age, Outcome) |&gt;\n  mutate(Outcome = recode(Outcome, `0` = \"No\", `1` = \"Yes\"))\n\nsleep_counts\n\n\n\n  \n\n\n\nThen, we can re-arrange the table of counts from a “long” into a “wide” layout, just for the sake of presenting the results in compact form:\n\nlibrary(tidyr)\n\nsleep_counts |&gt;\n  pivot_wider(names_from = Age, values_from = n) |&gt;\n  mutate(Outcome = recode(Outcome,\n                          \"No\" = \"Fewer than 7 hours\",\n                          \"Yes\" = \"7 hours or more\"\n                          )\n         )\n\n\n\n  \n\n\n\nThe results are also visualized in a scatter plot, showing the relationship between the proportion of respondents saying “7 hours or more” for each age group. To reproduce this scatter plot, we must first compute these conditional probabilities; in other words, we have to compute the proportion of respondents saying “7 hours or more”, given the respondent is a particular age.\nThere are several ways to do this, but for reasons that will become clear in later sections, the best approach for us is to create one column holding the number of “Yes” responses for each age group, and one column holding the number of “No” responses for each age group. The, we’ll compute the probability of saying “Yes, I get 7 or more hours of sleep a night”, given a respondent has a particular age value, by taking the number of “Yes” values in each row, and dividing by the sum of the “Yes” and “No” responses in that row:\n\nsleep_counts &lt;- sleep_counts |&gt;\n  pivot_wider(names_from = Outcome, values_from = n) |&gt;\n  mutate(prob_7_or_more = Yes / (Yes + No))\n\nsleep_counts\n\n\n\n  \n\n\n\nThen we can create a scatter plot showing the probability of responding “7 hours or more” for each age using ggplot,\n\nlibrary(ggplot2)\n\nggplot(data = sleep_counts,\n       mapping = aes(x=Age, y=prob_7_or_more)\n       ) +\n  geom_point() +\n  scale_x_continuous('Age', limits = c(0,40)) +\n  scale_y_continuous('Proportion Saying \"Yes\"', limits = c(0, 1))\n\n\n\n\nWe will take the author’s advice, and not fit a linear regression model to these data. As a consequence, we will not demonstrate how to re-create Figure 9.2, as this figure is included in the book only to illustrate what not to do when modeling a binary outcome variable.\n\n9.1.1 The Logistic Transformation\nRightfully, the next section begins with a visualization of the model you should fit to these data, a logistic regression model!\n\n\nWarning: The following aesthetics were dropped during statistical transformation: s, f\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nFigure 9.1: Reproducing Figure 9.3 from STAT2, which visualizes the predicted probability of sleeping 7 or more hours a night, given a teenager’s age, based on a logistic regression model.\n\n\n\n\nBut before we learn about how to visualize the logistic model shown in Figure 9.1, we’ll learn about how to fit a logistic model.\nTo fit a logistic model, we’ll need to use a new R function; we’ll need to use the glm() function (as a opposed to the lm() function). The “g” in “glm” is short for “generalized”; logistic regression models are said to be an example of a “generalized” linear model. Below, we fit a logistic regression model that uses a teenager’s age to predict the probability they get 7 or more hours of sleep a night:\n\nsleep_model &lt;- glm(cbind(Yes, No) ~ Age, data = sleep_counts,\n                   family = binomial\n                   )\n\nLet’s take a moment to point out what is similar, and dissimilar, from our previous work fitting linear regression models with the lm() function.\n\n\n\n\n\n\n\nSimilarity\nDissimilarity\n\n\n\n\nModel structure still specified with a formula\nLeft-hand side of formula uses two variables\n\n\nMust include data set as an argument\nMust also include a family argument\n\n\n\nOn the left hand side of our model formula, we have a two-column matrix: one column holds the number of “Yes” responses for each row in the data set, and one column holds the number of “No” responses for each row in the data set. If the data set you are using measures the binary categorical outcome using the number of observations that fell into each category, then you must supply both of the counts to the glm() function in order to fit the model.\nThe family=binomial argument is also crucial; this is what makes R understand that the two values on the left hand side of the formula don’t reflect two completely separate variables, but actually measure the number of observations falling into the “yes” and “no” response categories. In other words, the family=binomial variable is what tells R you are modeling a binary categorical outcome, instead of a continuous numeric outcome.\nFortunately, we can still find the fitted coefficients by summarizing the fitted model object:\n\nsummary(sleep_model)$coefficients\n\n              Estimate Std. Error   z value   Pr(&gt;|z|)\n(Intercept)  3.1186375 1.33374817  2.338251 0.01937425\nAge         -0.1513594 0.08234823 -1.838041 0.06605631\n\n\nThe regression table for a logistic regression tables looks extremely similar to the regression table for a linear regression, with the small difference that the hypothesis test for each coefficient is a based on a z distribution (i.e., a Normal distribution with mean=0 and standard deviation=1) instead of a t-distribution.\nAnd, we can still say that our estimated outcome is a linear function of our fitted coefficients:\n\\[\n\\hat{y} = 3.12 + -0.15 \\cdot Age\n\\tag{9.1}\\]\nBut, our \\(\\hat{y}\\) is something entirely new! It’s not the estimated number of “Yes” responses, the estimated number of “No” responses, or the estimated probability of “Yes” response. Here, \\(\\hat{y}\\) represents the estimated the log-odds of a “Yes” response:\n\\[\n\\hat{y} = ln\\big(\\frac{\\pi}{1-\\pi}\\big)\n\\tag{9.2}\\]\nSubstituting Equation 9.2 into Equation 9.1 shows us that a linear combination of the explanatory variables used in a logistic regression predicts the log-odds of a “success” from the outcome variable:\n\ncat(\"$$\\n\")\n\n$$\n\ncat(paste0(\"ln\\\\big(\\\\frac{\\\\pi}{1-\\\\pi}\\\\big) = \", betas[1], \" + \", betas[2], \" \\\\cdot Age\\n\"))\n\nln\\big(\\frac{\\pi}{1-\\pi}\\big) = 3.12 + -0.15 \\cdot Age\n\ncat(\"$$\\n\")\n\n$$\n\n\nBecause of this, the equation for a logistic regression model is sometimes expressed as \\(\\widehat{logit(y)} = b_0 + b_1 \\cdot x\\), since logit() is a commonly used abbreviation for the log-odds transformation.\nFigure 9.4 visualizes the observed data, and the logistic model’s predictions, on the log-odds scale. We can re-create this plot be 1) applying the logit transformation directly to our data, and 2) computing the model’s predictions across a fine grid of x-axis points using the broom::augment() function, and plotting the resulting predictions as a line.\nFirst, we transform the observed probabilities into log-odds:\n\nsleep_counts &lt;- sleep_counts |&gt;\n  mutate(log_odds = log(prob_7_or_more / (1-prob_7_or_more) ) )\n\nThe, compute the model’s predictions on the log-odds scale as well\n\nlibrary(broom)\n\npredicted_log_odds &lt;- augment(sleep_model,\n                              newdata = data.frame(Age = seq(0, 40, by=.1))\n                              )\npredicted_log_odds\n\n\n\n  \n\n\n\nFinally, we create a scatter plot using the log-odds, and draw a line representing the model’s predictions for each Age value along the x-axis. Notice that the predicted_log_odds data frame is passed to the geom_line() function as a layer-specific data frame.\n\nggplot(data = sleep_counts,\n       mapping = aes(x=Age, y=log_odds)\n       ) +\n  geom_point() +\n  geom_line(mapping = aes(y=.fitted),\n            data = predicted_log_odds,\n            color = \"blue\"\n            ) +\n  scale_x_continuous('Age', limits = c(0,40)) +\n  scale_y_continuous('Log-Odds of Saying \"Yes\"')\n\n\n\n\nFigure 9.2: Reproducing Figure 9.4 from STAT2, which visualizes the predicted log-odds of sleeping 7 or more hours a night, given a teenager’s age, based on a logistic regression model.\n\n\n\n\nHowever, visualizing your data and model on the log-odds scale is often not very informative, as the log-odds scale is a very abstract measurement scale. Usually, it is preferable to measure your model’s prediction on the probability scale. You can transform values on the log-odds scale into the probability scale using the logistic function:\n\\[\n\\frac{1}{1 + e^{\\hat{y}}}\n\\] where \\(\\hat{y} = b_0 + b_1 \\cdot x\\), i.e., where \\(\\hat{y}\\) are your model’s predictions on the log-odds scale.\nWe don’t have to apply this transformation “manually” before plotting - we can ask the augment() function to apply it for us by using the type.predict argument to ask it to compute the model’s predictions to be on the “response” scale (i.e., the same measurement scale as the original data).\n\npredicted_probabilities &lt;- augment(sleep_model, \n                                   newdata = data.frame(Age = seq(0, 40, by=.1)),\n                                   type.predict = \"response\"\n                                   )\npredicted_probabilities\n\n\n\n  \n\n\n\nNotice how the values in the .fitted are all between 0 and 1 - consistent with the probability scale of measurement. We can visualize these predictions, along with the observed probabilities, using a similar approach as taken in Figure 9.2.\n\nggplot(data = sleep_counts,\n       mapping = aes(x=Age, y=prob_7_or_more)\n       ) +\n  geom_point() +\n  geom_line(mapping = aes(y=.fitted),\n            data = predicted_probabilities,\n            color = \"blue\"\n            ) +\n  scale_x_continuous('Age', limits = c(0,40)) +\n  scale_y_continuous('Proportion Saying \"Yes\"', limits = c(0, 1))\n\n\n\n\nFigure 9.3: Reproducing Figure 9.3 from STAT2, which visualizes the predicted probability of sleeping 7 or more hours a night, given a teenager’s age, based on a logistic regression model.\n\n\n\n\n\n\n9.1.2 Example 9.4\nExample 9.4 explores a regression model that uses college GPA to predict the probability a medical school applicant is accepted to medical school. The purpose of this example in the text is to explain how to convert log-odds of acceptance into the probability of acceptance using the logistic function. However, here we focus on how to fit and visualize this model in R, because the MedGPA data set is structured differently than the LosingSleep data set.\n\ndata(\"MedGPA\")\nMedGPA\n\n\n\n  \n\n\n\nIn this data set, the outcome variable is once again a set of 0’s and 1’s, representing “not admitted” and “admitted”, respectively. In the previous example, with the LosingSleep data set, we converted these 0’s and 1’s to category labels, and counted the number of “Yes” answers for each age. This was a sensible way to summarize and explore the data, since the explanatory variable (age) was a discrete variable; it was natural to divide the “Yes” or “No” outcomes into groups based on the age variable.\nBut here, the explanatory variable is a continuous variable; there is no natural way to group the observations based on GPA when the GPA values are continuous values like 3.34, and 3.35, and 3.36. It doesn’t make conceptual sense to treat 3.34 as a separate group of GPA values than 3.35. At first glance, this seems to present a practical problem: how can we fit a logistic model in R if we can’t count the number of “success” and “failures” for each GPA value?\nLuckily, this step of counting the number of “success” and “failures” for each value of the explanatory variable isn’t a necessary step. We can use the “raw” 0 and 1 values as our outcome variable in our model formula!\n\nacceptance_logistic_model &lt;- glm(Acceptance ~ GPA, data = MedGPA,\n                                 family = binomial\n                                 )\nsummary(acceptance_logistic_model)$coefficients\n\n              Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -19.206503   5.628726 -3.412229 0.0006443386\nGPA           5.454166   1.579169  3.453821 0.0005527051\n\n\nThe family = binomial argument is once again necessary to tell R that these 0’ and 1’s don’t represent quantities, but actually values that represent different categorical outcomes. Let’s check that we obtain the same fitted regression mode that the authors do\n\nlibrary(equatiomatic)\nextract_eq(acceptance_logistic_model, use_coefs = TRUE)\n\n\\[\n\\log\\left[ \\frac { \\widehat{P( \\operatorname{Acceptance} = \\operatorname{1} )} }{ 1 - \\widehat{P( \\operatorname{Acceptance} = \\operatorname{1} )} } \\right] = -19.21 + 5.45(\\operatorname{GPA})\n\\]\n\n\nWe do! We can also re-create Figure 9.8b using the same technique we used to create Figure 9.3:\n\npredicted_probabilities &lt;- augment(acceptance_logistic_model, \n                                   newdata = data.frame(GPA = seq(2.7, 4, by=.01)),\n                                   type.predict = \"response\"\n                                   )\n\n\nggplot(data = MedGPA,\n       mapping = aes(x=GPA, y=Acceptance)\n       ) +\n  geom_jitter(height = .01) +\n  geom_line(mapping = aes(y=.fitted),\n            data = predicted_probabilities,\n            color = \"blue\"\n            ) +\n  scale_x_continuous('GPA', limits = c(2.7, 4),\n                     breaks = seq(2.8, 4, by=.2)\n                     ) +\n  scale_y_continuous('Probability of Acceptance')\n\n\n\n\nFigure 9.4: Reproducing Figure 9.8b from STAT2, which visualizes the predicted probability of being accepted to medical school, based on your college GPA.\n\n\n\n\nEach observed “success” is represented by a point at y=1, and each observed “failure’ is represented by a point at y=0. The logistic curve in blue shows how the estimated probability of acceptance increases as GPA increases."
  },
  {
    "objectID": "09_logistic_regression.html#logistic-regression-and-odds-ratios",
    "href": "09_logistic_regression.html#logistic-regression-and-odds-ratios",
    "title": "9  Logistic Regression",
    "section": "9.2 Logistic Regression and Odds Ratios",
    "text": "9.2 Logistic Regression and Odds Ratios"
  },
  {
    "objectID": "10_multiple_logistic_regression.html#overview",
    "href": "10_multiple_logistic_regression.html#overview",
    "title": "10  Multiple Logistic Regression",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nSection 10.1 serves to orient the reader to the conceptual similarities and differences between the different types of model fit throughout the books. Here, we’ll take the opportunity to the same, but focusing on the computational tools used to fit each type of model:\n\n\n\n\n\n\n\n\nModel\nR Function\nNotes\n\n\n\n\nSimple Linear Regression\nlm()\n\n\n\nMultiple Linear Regression\nlm()\n\n\n\nOne-Way ANOVA\nafex::car_aov()\n\n\n\nTwo-Way ANOVA\nafex::car_aov()\n\n\n\nSimple Logistic Regression\nglm()\nMust include family=binomial argument\n\n\nMultiple Linear Regression\nglm()\nMust include family=binomial argument"
  },
  {
    "objectID": "10_multiple_logistic_regression.html#choosing-fitting-and-interpreting-models",
    "href": "10_multiple_logistic_regression.html#choosing-fitting-and-interpreting-models",
    "title": "10  Multiple Logistic Regression",
    "section": "10.2 Choosing, Fitting, and Interpreting Models",
    "text": "10.2 Choosing, Fitting, and Interpreting Models\nThe first multiple logistic regression presented uses the Eyes data set, which reports the results of an experiment measuring the pupil dilation of both male and female participants while they view images of nude males and females. The model presented here estimates the likelihood that the participant is “Gay”, based the change in average pupil dilation between viewing male and females images, and the sex of the participant. Here, “Gay” was not a self-identification made by the participant; “Gay” was operationalized to mean the participant received a composite score of 4 or higher on based on their self-reported answers to several Kinsey-scale questions).1\n\nlibrary(Stat2Data)\ndata(\"Eyes\")\nEyes\n\n\n\n  \n\n\n\nNotice that we have our data stored at the level of individuals, with a 0 or a 1 for each individual “failure” or “success” observed (as opposed to having our data stored as counts, e.g., 10 failure and 19 successes).\nBefore continuing with an analysis of these data, it is worth noting using statistical models to predict an individual’s sexual preferences is an activity fraught with ethical peril. It is not difficult to imagine a situation where a device like a smart phone measures a person’s pupil dilation while they consume visual media, and uses other classification techniques to measure whether the person is viewing images of males or females. These measurements could then be used identify individuals likely to be homosexual, and used for a variety of purposes from the annoying (selling targeted ads) to the dangerous (an authoritarian government seeking to target gay individuals).\nIn this case, the data were collected in an experiment conducted under the supervision of an Institutional Review Board (IRB) and participants gave informed consent about what measurements would be collected and for what purposes. But, we should always be mindful of ways that the data we collect, and data analysis techniques we develop, present the potential for harmful misuse.\n\n10.2.1 Example 10.1: The eyes have it\nOne of first ways the Eyes data are explored is with an empirical logit plot; the data are grouped in to discrete “bins” based on the quantiles of the of pupil dilation scores (-1.1 to -.301, -3 to -.074, 0.073 to .07, and .071 to 1.3), and the log-odds of being “Gay” is calculated within each bin. These log-odds are presented in Table 10.3, and Figure 10.2; these items are re-created below as an example of how to create an empirical logit plot.\n\nlibrary(dplyr)\n\nbin_upper_bounds &lt;-  quantile(Eyes$DilateDiff, p = seq(.25, 1, by=.25))\nbin_upper_bounds\n\n        25%         50%         75%        100% \n-0.30081510 -0.07364091  0.07065124  1.20367290 \n\n## We place observations into bins by comparing each observation to the upper\n## boundary of all 4 intervals.\n\n## First, we measure whether or not the observation exceeds the upper boundary\n## of each bin. Then, we figure out the position of the smallest boundary value\n## **NOT** exceeded; This is the bin the observation belongs to.\n##\n## For example, if an observation exceeds the boundary of the first two bins,\n## but not the third or fourth, the third boundary is the smallest boundary values not \n## exceeded. Thus, it belongs to the third bin.\n\nEyes_binned_odds &lt;- Eyes |&gt;\n  rowwise() |&gt;\n  mutate(bin_number = min(which(DilateDiff &lt;= bin_upper_bounds))) |&gt; \n  group_by(bin_number) |&gt;\n  summarize(N = n(),\n            avg_dilation_diff = mean(DilateDiff),\n            n_gay = sum(Gay),\n            p_gay = mean(Gay),\n            log_odds_gay = p_gay / (1 - p_gay)\n            )\n\nEyes_binned_odds\n\n\n\n  \n\n\n\nWe can make an empirical logit plot by construction a scatter plot of the log-odds of being “Gay” against the average dilation difference score in each bin:\n\nlibrary(ggplot2)\n\nggplot(Eyes_binned_odds,\n       aes(x = avg_dilation_diff, y = log_odds_gay)\n       ) + \n  geom_point() +\n  geom_smooth(method = lm, se=FALSE, formula = y~x)\n\n\n\n\nIt is worth pointing out that while Figure 10.2, Figure 10.3, and Figure 10.4 all have log-odds on the y-axis, and lines of ‘best fit’ drawn in them, none of these plots visualize the predictions of a logistic regression model. These plots all represent linear regression models fit to the log-odds of “success”; this is useful for exploratory purposes, but not suitable for a final analysis.\nTo obtain an unbiased estimate of the log-odds of “success”, we need to fit a multiple logistic regression model to the raw “success” and “failure” observations. This is only slightly more difficult that fitting a “simple” logistic regression model; all we are required to do differently is literally add a second explanatory variable to the model formula in R!\nOne bit of data wrangling is helpful first; replacing the 0’s representing female participants with the word “Female”, and replacing the 1’s representing male participants with the word “Female”\n\nEyes &lt;- Eyes |&gt;\n  mutate(Sex = recode(SexMale, `0`=\"Female\", `1`=\"Male\"))\n\ngay_dilation_model &lt;- glm(Gay ~ DilateDiff + Sex, data = Eyes,\n                          family = binomial\n                          )\nsummary(gay_dilation_model)$coefficients\n\n             Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -1.265639  0.3820190 -3.313025 9.229257e-04\nDilateDiff   3.290778  0.8180317  4.022800 5.751032e-05\nSexMale      1.289656  0.4952921  2.603830 9.218853e-03\n\n\nFigure 10.5 shows the predictions of the fitted model in both the probability and logit forms. We can reproduce these figures using the same techniques from Chapter 9.1:\n\nCreate a “reference grid”, representing every combination of the explanatory variables we wish to obtain a prediction for\nUse the broom::augment() function to obtain a prediction for every value in the grid\nPlot these predictions using geom_line()\n\nThe only added complication is that here, we must use the expand.grid() function to create our “reference grid”, since we care about obtaining a prediction for many, closely spaced values of the DilateDiff variable, for both male and female participants.\n\nref_grid &lt;- expand.grid(DilateDiff = seq(from = min(Eyes$DilateDiff),\n                                         to = max(Eyes$DilateDiff),\n                                         by = .01\n                                         ),\n                        Sex = c(\"Female\", \"Male\")\n                        )\n\n\n\n\nhead(ref_grid)\n\n\n\n  \n\n\n\n\n\ntail(ref_grid)\n\n\n\n  \n\n\n\n\n\nWith our reference grid created, we can move on to step 2 and 3 to re-created Panel B:\n\nlibrary(broom)\n\npredicted_odds &lt;- augment(gay_dilation_model, newdata = ref_grid)\npredicted_odds\n\n\n\n  \n\n\n\n\nggplot(data = predicted_odds, aes(x=DilateDiff, y=.fitted, color=Sex)) +\n  geom_line()\n\n\n\n\nPanel A can be re-created by asking the augment() function to return it’s predictions on the \"response\" scale (i.e., with the log-odds squashed between 0 and 1, using the logistic transformation):\n\npredicted_prob &lt;- augment(gay_dilation_model, newdata = ref_grid,\n                          type.predict = \"response\"\n                          )\npredicted_prob\n\n\n\n  \n\n\n\n\nggplot(data = predicted_prob, aes(x=DilateDiff, y=.fitted, color=Sex)) +\n  geom_line()\n\n\n\n\nWe could even add the raw data to this plot, since the raw data were measured on the 0/1 scale\n\nggplot(data = predicted_prob, aes(x=DilateDiff, y=.fitted, color=Sex)) +\n  geom_line() +\n  geom_jitter(data = Eyes,\n              mapping = aes(y = Gay),\n              height = .01\n              )\n\n\n\n\n\n\n10.2.2 Example 10.2: Medical school admissions"
  },
  {
    "objectID": "10_multiple_logistic_regression.html#footnotes",
    "href": "10_multiple_logistic_regression.html#footnotes",
    "title": "10  Multiple Logistic Regression",
    "section": "",
    "text": "Ironically, the Kinsey scale was originally developed as a way of understanding sexual preferences without explicitly categorizing people as exclusively heterosexual, homosexual or bisexual!↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, Daniel, Andrew Heiss, and Jay Sumners. 2022. Equatiomatic:\nTransform Models into ’LaTeX’ Equations. https://CRAN.R-project.org/package=equatiomatic.\n\n\nCannon, A. R., G. W. Cobb, B. A. Hartlaub, J. M. Legler, R. H. Lock, T.\nL. Moore, A. J. Rossman, and J. A. Witmer. 2018. STAT2: Modeling\nwith Regression and ANOVA. Macmillan Learning. https://www.macmillanlearning.com/college/us/product/STAT2/p/1319054072.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for\nTidyverse-Friendly Statistical Inference.” Journal of Open\nSource Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion\nto Applied Regression. Third. Thousand Oaks CA: Sage.\nhttps://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nKim, Albert Y., Chester Ismay, and Max Kuhn. 2021. “Take a\nModerndive into Introductory Linear Regression with r.” The\nJournal of Open Source Education 4 (41, 115). https://doi.org/10.21105/jose.00115.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip\nWaggoner, and Dominique Makowski. 2021. “performance: An R Package for\nAssessment, Comparison and Testing of Statistical Models.”\nJournal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. Broom: Convert\nStatistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz\nMarbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally:\nExtension to ’Ggplot2’. https://CRAN.R-project.org/package=GGally.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\nDplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  }
]